# Produce dan Consume menggunakan Schema Registry

## Produce dan Consume menggunakan Schema Registry di Java

Pertama, pastikan status schema registry pada vm confluent sudah running dan port 8081 bisa di-telnet dari client:

```
sudo systemctl status confluent-schema-registry
```

![image](https://github.com/user-attachments/assets/0ba14b8e-575d-495e-bb43-097ed45a34e7)

Pada file `pom.xml`, tambahkan library berikut di bagian Dependency:

```
<!-- Confluent Kafka Avro Serializer -->
<dependency>
        <groupId>io.confluent</groupId>
        <artifactId>kafka-avro-serializer</artifactId>
        <version>7.0.0</version>
</dependency>

<!-- Avro -->
<dependency>
    <groupId>org.apache.avro</groupId>
    <artifactId>avro</artifactId>
    <version>1.10.2</version>
</dependency>
```

Dan tambahkan Confluent Maven Repositories:

```
<repositories>
        <repository>
            <id>confluent</id>
            <url>https://packages.confluent.io/maven/</url>
        </repository>
</repositories>
```

### Producer

Ubah kode di file `Producer.java` hingga menjadi seperti berikut:

```
package com.example;

import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.StringSerializer;

import io.confluent.kafka.serializers.KafkaAvroSerializer;
import org.apache.avro.Schema;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericRecord;

import java.util.Properties;

public class Producer {
    public static void main(String[] args) {
        String bootstrapServers = "10.100.13.149:9092";
        String schemaRegistryUrl = "http://10.100.13.149:8081";
        String topic = "test_topic_schema";

        // Avro schema definition
        String userSchema = 
        "{"
            + "\"type\": \"record\","
            + "\"name\": \"User\","
            + "\"fields\": ["
                + "{ \"name\": \"name\", \"type\": \"string\" },"
                + "{ \"name\": \"age\", \"type\": \"int\" }"
            + "]"
        + "}";

        Schema.Parser parser = new Schema.Parser();
        Schema schema = parser.parse(userSchema);

        // Create a GenericRecord with the schema
        GenericRecord avroRecord = new GenericData.Record(schema);
        avroRecord.put("name", "Ivy");
        avroRecord.put("age", 21);

        // Producer properties
        Properties properties = new Properties();
        properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class.getName());
        properties.setProperty("schema.registry.url", schemaRegistryUrl);  // Setting Schema Registry URL

        // Create the producer
        KafkaProducer<String, GenericRecord> producer = new KafkaProducer<>(properties);

        // Send data
        ProducerRecord<String, GenericRecord> record = new ProducerRecord<>(topic, "key", avroRecord);
        producer.send(record, (metadata, exception) -> {
            if (exception == null) {
                System.out.println("Sent message with offset: " + metadata.offset());
            } else {
                exception.printStackTrace();
            }
        });

        // Flush and close producer
        producer.flush();
        producer.close();
    }
}
```

Berdasarkan kode di atas, berikut adalah library-library tambahan yang diimport untuk menggunakan schema registry.

```
import io.confluent.kafka.serializers.KafkaAvroSerializer;
import org.apache.avro.Schema;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericRecord;
```

Berikut adalah konfigurasi yang ditambahkan:

```
String schemaRegistryUrl = "http://10.100.13.149:8081";

// Avro schema definition
String userSchema = 
"{"
    + "\"type\": \"record\","
    + "\"name\": \"User\","
    + "\"fields\": ["
        + "{ \"name\": \"name\", \"type\": \"string\", \"default\": \"Unknown\"},"
        + "{ \"name\": \"age\", \"type\": \"int\", \"default\": 0 }"
    + "]"
+ "}";
Schema.Parser parser = new Schema.Parser();
Schema schema = parser.parse(userSchema);

// Create a GenericRecord with the schema
GenericRecord avroRecord = new GenericData.Record(schema);
avroRecord.put("name", "Ivy");
avroRecord.put("age", 21);
```

Dengan kode di atas, message ditetapkan untuk mengikuti format userSchema yang menyertakan nama dan umur. Lalu, pastikan value yang dikirim diambil dari avroRecord, bukan string yang sebelumnya.

```
properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class.getName());
properties.setProperty("schema.registry.url", schemaRegistryUrl);  // Setting Schema Registry URL

// Create the producer
KafkaProducer<String, GenericRecord> producer = new KafkaProducer<>(properties);

// Send data
ProducerRecord<String, GenericRecord> record = new ProducerRecord<>(topic, "key", avroRecord);
```

### Consumer

Ubah kode di file `Consumer.java` menjadi berikut:

```
package com.example;

import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.serialization.StringDeserializer;
import io.confluent.kafka.serializers.KafkaAvroDeserializer;
import org.apache.avro.generic.GenericRecord;

import java.time.Duration;
import java.util.Collections;
import java.util.Properties;

public class Consumer {
    public static void main(String[] args) {
        String bootstrapServers = "10.100.13.149:9092";
        String schemaRegistryUrl = "http://10.100.13.149:8081";
        String groupId = "java-group";
        String topic = "test_topic_schema";

        // Consumer properties
        Properties properties = new Properties();
        properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, groupId);
        properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class.getName()); // Avro Deserializer
        properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        properties.setProperty("schema.registry.url", schemaRegistryUrl); // Schema Registry URL
        properties.setProperty("specific.avro.reader", "false"); // Use GenericRecord

        // Create the consumer
        KafkaConsumer<String, GenericRecord> consumer = new KafkaConsumer<>(properties);

        // Subscribe to the topic
        consumer.subscribe(Collections.singleton(topic));

        // Poll for new data
        while (true) {
            ConsumerRecords<String, GenericRecord> records = consumer.poll(Duration.ofMillis(100));
            for (ConsumerRecord<String, GenericRecord> record : records) {
                String name = record.value().get("name").toString();
                int age = (int) record.value().get("age");

                System.out.println("Received message: Name = " + name + ", Age = " + age + 
                                   ", Offset: " + record.offset());
            }
        }
    }
}

```

Berdasarkan kode di atas, berikut adalah library-library tambahan yang diimport untuk menggunakan schema registry.

```
import io.confluent.kafka.serializers.KafkaAvroDeserializer;
import org.apache.avro.generic.GenericRecord;
```

Berikut adalah konfigurasi yang ditambahkan:

```
String schemaRegistryUrl = "http://10.100.13.149:8081";

properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class.getName()); // Avro Deserializer
properties.setProperty("schema.registry.url", schemaRegistryUrl); // Schema Registry URL
properties.setProperty("specific.avro.reader", "false"); // Use GenericRecord
```

Dengan kode di atas, message ditetapkan di-deserialize menggunakan schema registry. Lalu, pastikan polling message berupa GenericRecord yang menyertakan nama dan umur.

```
// Create the consumer
KafkaConsumer<String, GenericRecord> consumer = new KafkaConsumer<>(properties);

// Subscribe to the topic
consumer.subscribe(Collections.singleton(topic));

// Poll for new data
while (true) {
    ConsumerRecords<String, GenericRecord> records = consumer.poll(Duration.ofMillis(100));
    for (ConsumerRecord<String, GenericRecord> record : records) {
        String name = record.value().get("name").toString();
        int age = (int) record.value().get("age");

        System.out.println("Received message: Name = " + name + ", Age = " + age + 
                           ", Offset: " + record.offset());
    }
}
```

Jalankan `mvn clean package` dan jalankan Producer dan consumer:

```
java -cp target/kafka-producer-consumer-1.0-SNAPSHOT-shaded.jar com.example.Producer
```

![image](https://github.com/user-attachments/assets/f274c236-49d0-4cc8-a356-e3841fbca4a9)

```
java -cp target/kafka-producer-consumer-1.0-SNAPSHOT-shaded.jar com.example.Consumer
```

![image](https://github.com/user-attachments/assets/06bff178-2cf3-482f-b41e-cfcf2cf9f915)

## Notes

### Lambda

1. Apa fungsi `->` (lambda), apa keuntungan dan kekurangannya?

```
producer.send(record, (metadata, exception) -> {
    if (exception == null) {
        System.out.println("Sent message with offset: " + metadata.offset());
    } else {
        exception.printStackTrace();
    }
});
```

Lambda di Java adalah ekspresi yang memungkinkan untuk mendefinisikan fungsi anonim (fungsi tanpa nama) yang dapat digunakan sebagai parameter atau sebagai return value dari suatu method. Dalam kode di atas, ada dua kondisi yang ditangani: jika tidak ada exception, ia mencetak offset. Jika ada exception, ia menampilkan stack trace dari error tersebut.

Contoh penggunaan lambda untuk menggantikan class anonim sebelum lambda:

```
new Thread(new Runnable() {
    @Override
    public void run() {
        System.out.println("Hello World");
    }
}).start();
```

Setelah menggunakan lambda:

```
new Thread(() -> System.out.println("Hello World")).start();
```

Keuntungan Lambda:
- Kode lebih singkat dan bersih
- Lebih mudah dibaca. Dengan lambda, fokus hanya pada logika, tanpa harus mendefinisikan implementasi class tambahan.
- Efisiensi kode dengan mengurangi kompleksitas dan ukuran kode yang diperlukan untuk tugas sederhana.
  
Kerugian Lambda:
- Debugging lebih sulit karena lambda tidak memiliki nama (anonymous), saat error, akan lebih sulit untuk melacak dan mengidentifikasi dalam call stack.
- Jika logika di dalam lambda terlalu kompleks, kode malah menjadi lebih sulit dimengerti daripada jika menggunakan class anonim atau implementasi method biasa.

### Cara mengirim Specific Record

Specific Record mengharuskan untuk membuat class Java dari skema Avro terlebih dahulu. Class ini menyediakan akses ke field skema, sehingga dapat berinteraksi dengan record dengan cara yang lebih alami menggunakan getters dan setters. Ideal ketika memiliki skema tetap dan lebih memilih keamanan, atau ingin kode Java mengikuti struktur skema Avro dengan cermat.

Pertama, simpan skema Avro Anda ke file .avsc. Misalnya, simpan skema berikut sebagai User.avsc dalam direktory `/src/main/avro`:

```
{
  "type": "record",
  "name": "User",
  "namespace": "com.example.avro",
  "fields": [
    { "name": "name", "type": "string", "default": "Unknown" },
    { "name": "age", "type": "int", "default": 0 }
  ]
}
```

Tambahkan plugin Avro Maven ke pom.xml untuk generasi class Java dalam proses build:

```
<plugin>
    <groupId>org.apache.avro</groupId>
    <artifactId>avro-maven-plugin</artifactId>
    <version>1.11.1</version>
    <executions>
        <execution>
            <goals>
                <goal>schema</goal>
            </goals>
            <configuration>
                <sourceDirectory>${project.basedir}/src/main/avro</sourceDirectory>
                <outputDirectory>${project.basedir}/src/main/java</outputDirectory>
            </configuration>
        </execution>
    </executions>
</plugin>
```

Compile:

```
mvn compile
```

Perintah ini mengkompilasi code source proyek. Perintah ini, berbeda dari `mvn package`, tidak mempackage kode yang dikompilasi ke dalam format yang dapat didistribusikan (seperti file .jar atau .war). Perintah ini hanya menangani fase kompilasi, yang berarti dependensi diselesaikan, dan kode diperiksa untuk kesalahan kompilasi.

Dengan class User yang dihasilkan, perbarui kode Producer Kafka untuk menggunakan record tertentu:

```
package com.example;

import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.StringSerializer;

import io.confluent.kafka.serializers.KafkaAvroSerializer;
import org.apache.avro.Schema;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericRecord;
import com.example.avro.User;

import java.util.Properties;

public class Producer {
    public static void main(String[] args) {
        String bootstrapServers = "10.100.13.149:9092";
        String schemaRegistryUrl = "http://10.100.13.149:8081";
        String topic = "test_topic_schema";

        // Avro schema definition
        // String userSchema = 
        // "{"
        //     + "\"type\": \"record\","
        //     + "\"name\": \"User\","
        //     + "\"fields\": ["
        //         + "{ \"name\": \"name\", \"type\": \"string\", \"default\": \"Unknown\"},"
        //         + "{ \"name\": \"age\", \"type\": \"int\", \"default\": 0 }"
        //     + "]"
        // + "}";

        // Schema.Parser parser = new Schema.Parser();
        // Schema schema = parser.parse(userSchema);

        // Create a GenericRecord with the schema
        // GenericRecord avroRecord = new GenericData.Record(schema);
        // avroRecord.put("name", "Ivy");
        // avroRecord.put("age", 21);

        User user = User.newBuilder()
                        .setName("Ivyna")
                        .setAge(21)
                        .build();

        // Producer properties
        Properties properties = new Properties();
        properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class.getName());
        properties.setProperty("schema.registry.url", schemaRegistryUrl);  // Setting Schema Registry URL

        // Create the producer
        KafkaProducer<String, User> producer = new KafkaProducer<>(properties);

        // Send data
        ProducerRecord<String, User> record = new ProducerRecord<>(topic, "key", user);
        producer.send(record, (metadata, exception) -> {
            if (exception == null) {
                System.out.println("Sent message with offset: " + metadata.offset());
            } else {
                exception.printStackTrace();
            }
        });

        // Flush and close producer
        producer.flush();
        producer.close();
    }
}
```

Dalam kode yang diperbarui, import schema User:

```
import com.example.avro.User;
```

Build instance user menggunakan builder pattern Avro:

```
User user = User.newBuilder()
                        .setName("Ivyna")
                        .setAge(21)
                        .build();
```

Ubah tipe parameter dari GenericRecord menjadi User:

```
 // Create the producer
KafkaProducer<String, User> producer = new KafkaProducer<>(properties);

// Send data
ProducerRecord<String, User> record = new ProducerRecord<>(topic, "key", user);
```

Jalankan mvn clean package dan jalankan Producer:

```
java -cp target/kafka-producer-consumer-1.0-SNAPSHOT-shaded.jar com.example.Producer
```

![image](https://github.com/user-attachments/assets/51739f0f-0d30-4b38-b94e-e542d74ae7e6)


Cek jika message masuk ke broker menggunakan Consumer:

```
java -cp target/kafka-producer-consumer-1.0-SNAPSHOT-shaded.jar com.example.Consumer
```

![image](https://github.com/user-attachments/assets/11aa1aec-868f-4654-ba1e-3a3d8510cf6f)

### Memindahkan parameter di file properties

Buat file bernama producer-config.properties di direktori src/main/resources. Tambahkan value konfigurasi ke file ini:

```
bootstrap.servers=10.100.13.149:9092
schema.registry.url=http://10.100.13.149:8081
key.serializer=org.apache.kafka.common.serialization.StringSerializer
value.serializer=io.confluent.kafka.serializers.KafkaAvroSerializer
topic=test_topic_schema
```

Dalam kode Producer java, import java.util.Properties dan InputStream.

```
import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStream;
```

Ubah kode inisialisasi variable di producer java supaya dia menginput value dari file yang telah dibuat.

```
// Load properties from configuration file
Properties properties = new Properties();
try (InputStream input = new FileInputStream("src/main/resources/producer-config.properties")) {
    properties.load(input);
} catch (IOException ex) {
    ex.printStackTrace();
}

String bootstrapServers = properties.getProperty("bootstrap.servers");
String schemaRegistryUrl = properties.getProperty("schema.registry.url");
String topic = properties.getProperty("topic");
```

```
properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, paramProperties.getProperty("key.serializer"));
properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, paramProperties.getProperty("value.serializer"));
```

Sekarang kode produser Java menjadi seperti ini:

```
package com.example;

import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.StringSerializer;

import io.confluent.kafka.serializers.KafkaAvroSerializer;
import org.apache.avro.Schema;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericRecord;
import com.example.avro.User;

import java.util.Properties;

public class Producer {
    public static void main(String[] args) {
        // String bootstrapServers = "10.100.13.149:9092";
        // String schemaRegistryUrl = "http://10.100.13.149:8081";
        // String topic = "test_topic_schema";

        // Load properties from configuration file
        Properties paramProperties = new Properties();
        try (InputStream input = Producer.class.getClassLoader().getResourceAsStream("producer-config.properties")) {
            paramProperties.load(input);
        } catch (IOException ex) {
            ex.printStackTrace();
        }

        String bootstrapServers = paramProperties.getProperty("bootstrap.servers");
        String schemaRegistryUrl = paramProperties.getProperty("schema.registry.url");
        String topic = paramProperties.getProperty("topic");

        // Avro schema definition
        // String userSchema = 
        // "{"
        //     + "\"type\": \"record\","
        //     + "\"name\": \"User\","
        //     + "\"fields\": ["
        //         + "{ \"name\": \"name\", \"type\": \"string\", \"default\": \"Unknown\"},"
        //         + "{ \"name\": \"age\", \"type\": \"int\", \"default\": 0 }"
        //     + "]"
        // + "}";

        // Schema.Parser parser = new Schema.Parser();
        // Schema schema = parser.parse(userSchema);

        // Create a GenericRecord with the schema
        // GenericRecord avroRecord = new GenericData.Record(schema);
        // avroRecord.put("name", "Ivy");
        // avroRecord.put("age", 21);

        User user = User.newBuilder()
                        .setName("Ivyna Johansen")
                        .setAge(21)
                        .build();

        // Producer properties
        Properties properties = new Properties();
        properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        producerProperties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, paramProperties.getProperty("key.serializer"));
        producerProperties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, paramProperties.getProperty("value.serializer"));
        properties.setProperty("schema.registry.url", schemaRegistryUrl);  // Setting Schema Registry URL

        // Create the producer
        KafkaProducer<String, User> producer = new KafkaProducer<>(properties);

        // Send data
        ProducerRecord<String, User> record = new ProducerRecord<>(topic, "key", user);
        producer.send(record, (metadata, exception) -> {
            if (exception == null) {
                System.out.println("Sent message with offset: " + metadata.offset());
            } else {
                exception.printStackTrace();
            }
        });

        // Flush and close producer
        producer.flush();
        producer.close();
    }
}
```

Jalankan Producer:

![image](https://github.com/user-attachments/assets/944f319f-7ec8-4450-a097-91402788a0d5)

Lakukan yang sama pada consumer. Buat file producer-config.properties yang isinya:

```
bootstrap.servers=10.100.13.149:9092
schema.registry.url=http://10.100.13.149:8081
key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
value.deserializer=io.confluent.kafka.serializers.KafkaAvroDeserializer
topic=test_topic_schema
group.id=java-group
```

Ubah kode java consumer menjadi berikut:

```
package com.example;

import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.serialization.StringDeserializer;
import io.confluent.kafka.serializers.KafkaAvroDeserializer;
import org.apache.avro.generic.GenericRecord;
import com.example.avro.User;

import java.io.IOException;
import java.io.InputStream;

import java.time.Duration;
import java.util.Collections;
import java.util.Properties;

public class Consumer {
    public static void main(String[] args) {
        // String bootstrapServers = "10.100.13.149:9092";
        // String schemaRegistryUrl = "http://10.100.13.149:8081";
        // String groupId = "java-group";
        // String topic = "test_topic_schema";

        Properties paramProperties = new Properties();
        try (InputStream input = Consumer.class.getClassLoader().getResourceAsStream("consumer-config.properties")) {
            paramProperties.load(input);
        } catch (IOException ex) {
            ex.printStackTrace();
        }

        String bootstrapServers = paramProperties.getProperty("bootstrap.servers");
        String schemaRegistryUrl = paramProperties.getProperty("schema.registry.url");
        String topic = paramProperties.getProperty("topic");
        String groupId = paramProperties.getProperty("group.id");

        // Consumer properties
        Properties properties = new Properties();
        properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, groupId);
        properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, paramProperties.getProperty("key.deserializer"));
        properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, paramProperties.getProperty("value.deserializer")); // Avro Deserializer
        properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        properties.setProperty("schema.registry.url", schemaRegistryUrl); // Schema Registry URL
        properties.setProperty("specific.avro.reader", "true"); 

        // Create the consumer
        KafkaConsumer<String, User> consumer = new KafkaConsumer<>(properties);

        // Subscribe to the topic
        consumer.subscribe(Collections.singleton(topic));

        // Poll for new data
        while (true) {
            ConsumerRecords<String, User> records = consumer.poll(Duration.ofMillis(100));
            for (ConsumerRecord<String, User> record : records) {
                User user = record.value(); // Deserialize directly into User object
                String name = user.getName().toString(); // Convert CharSequence to String
                int age = user.getAge();

                System.out.println("Received message: Name = " + name + ", Age = " + age + 
                                   ", Offset: " + record.offset());
            }
        }
    }
}
```

Jalankan file consumer:

![image](https://github.com/user-attachments/assets/db122213-7e58-4c9b-b201-e090de43b02b)


