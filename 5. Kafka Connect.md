# Kafka Connect

## 1. Install Connector

Untuk menginstall Kafka Connector, pergi ke link https://www.confluent.io/hub/ dan download connector yang diinginkan, contohnya Oracle CDC Source Connector. Untuk mencari tahu path extraction zip file, cari `connect-distributed.properties` dimana Kafka terinstalasi.

![image](https://github.com/ivynajohansen/belajar-confluent/assets/83331802/c8962881-3c10-4c14-862d-51c10ff81f68)

Lalu, extract zip file ke `/usr/share/confluent-hub-components` dan restart service kafka connect dengan:

```
sudo systemctl restart confluent-kafka-connect
```

## 2. Setup Kafka Connector untuk Source

Navigasi ke Confluent Control Center (C3), klik pilihan `Connect` di navigation bar, klik `connect-default`, dan Add Connector baru yaitu Oracle CDC Source Connector. Setelah itu isi konfigurasi berdasarkan value berikut:

```
{
  "name": "cdc-source-oracle-pdb",
  "config": {
    "connector.class": "io.confluent.connect.oracle.cdc.OracleCdcSourceConnector",
    "tasks.max": "1",
    "key.converter": "io.confluent.connect.avro.AvroConverter",
    "value.converter": "io.confluent.connect.avro.AvroConverter",
    "oracle.server": "10.100.13.32",
    "oracle.port": "1521",
    "oracle.sid": "ora19",
    "oracle.username": "sys as sysdba",
    "oracle.password": "@Password123",
    "redo.log.consumer.bootstrap.servers": "worker2.k8s.alldataint.com:9092",
    "table.inclusion.regex": "ora19[.]IVYNA[.](STATIONERIES)",
    "key.converter.schema.registry.url": "http://worker2.k8s.alldataint.com:8081",
    "value.converter.schema.registry.url": "http://worker2.k8s.alldataint.com:8081"
  }
}
```

Konfigurasi ini akan membuat database ora19 Oracle pada `10.100.13.32` schema `IVYNA` tabel `STATIIONERIES` untuk dijadikan source di topic `ora19.IVYNA.STATIONERIES`.

Pastikan Confluent Schema Registry sudah running dan topic source memiliki schema:

![image](https://github.com/ivynajohansen/belajar-confluent/assets/83331802/94fe1690-7d0b-48cf-96cf-d2591333c220)


## 3. Setup Kafka Connector untuk Sink

### a. Setup Postgres

Install Postgres 13 di CentOS:

```
sudo yum install postgresql13-server postgresql13
sudo yum install pgxnclient
sudo yum install postgresql-devel
ssudo /usr/pgsql-13/bin/postgresql-13-setup initdb
sudo systemctl start postgresql
sudo systemctl enable postgresql
```

Navigasi ke `var/lib/pgsql/13/data/postgresql.conf` dan uncomment line berikut:

```
listen_addresses = '*'
```

Tambahkan line berikut di pg_hba_conf:

```
host    all             all             0.0.0.0/0               md5
```

Restart postgresql

```
sudo systemctl restart postgresql
```

### b. Buat Database dan User

Akses ke CLI Postgres:

```
sudo -i -u postgres
export PATH=$PATH:/usr/pgsql-13/bin
psql
```

Buat Database dan User:

```
CREATE DATABASE postgres_sink;
CREATE USER ivyna WITH PASSWORD 'ivyna';
GRANT ALL PRIVILEGES ON DATABASE postgres_sink TO ivyna;
```

### c. Konfigurasi Kafka Connect

Gunakan JDBC Sink Connector di C3 dan konfigurasi connector berdasarkan value-value berikut:

```
{
  "name": "postgres-sink-connector",
  "config": {
    "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
    "tasks.max": "2",
    "key.converter": "io.confluent.connect.avro.AvroConverter",
    "value.converter": "io.confluent.connect.avro.AvroConverter",
    "transforms": "InsertField",
    "topics": "ora19.IVYNA.STATIONERIES",
    "transforms.InsertField.type": "org.apache.kafka.connect.transforms.InsertField$Value",
    "transforms.InsertField.offset.field": "kafka_offset",
    "transforms.InsertField.timestamp.field": "kafka_timestamp",
    "connection.url": "jdbc:postgresql://10.100.13.26:5432/postgres_sink",
    "connection.user": "ivyna",
    "connection.password": "ivyna",
    "dialect.name": "PostgreSqlDatabaseDialect",
    "insert.mode": "insert",
    "table.name.format": "public.stationeries",
    "pk.mode": "none",
    "key.converter.schema.registry.url": "http://worker2.k8s.alldataint.com:8081",
    "value.converter.schema.registry.url": "http://worker2.k8s.alldataint.com:8081",
    "auto.create": "true",
    "auto.evolve": "true"
  }
}
```

Setelah Kafka Connect berhasil running, data dari topic akan berhasil diambil oleh sink.

![image](https://github.com/ivynajohansen/belajar-confluent/assets/83331802/99636b3f-c8a2-4215-b0fd-da8a12c400ab)


## 4. Postgresql sebagai CDC Source

### a. Debezium PostgreSQL CDC Source Connector

Install Connector PostgreSQL di `https://www.confluent.io/hub/debezium/debezium-connector-postgresql` dan extract zip file dalam `/usr/share/confluent-hub-components`.

### b. Edit postgresql.conf file

Cari `wal_level` di `postgresql.conf` dan ubah atau uncomment line menjadi:

```
wal_level = logical
```

Lalu ke CLI psql untuk memberikan permission pada user `ivyna`:

```
ALTER ROLE ivyna REPLICATION LOGIN;
```

### c. Install Plugin Decoderbufs

Download zip file untuk Decoderbufs dari github repository:

```
wget https://github.com/debezium/postgres-decoderbufs/archive/refs/tags/v1.7.0.Final.tar.gz
```

Extract zip file:

```
tar -zxvf v1.7.0.Final.tar.gz
```

Navigasi ke folder dari hasil extract:

```
cd postgres-decoderbufs-1.7.0.Final
```

Edit Makefile

```
vim Makefile
```

Ubah line berikut:

```
PG_CONFIG ?= usr/pgsql-13/bin/pg_config
```


