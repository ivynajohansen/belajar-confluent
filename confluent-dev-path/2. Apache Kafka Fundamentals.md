# Apache Kafka Fundamentals

## Producer

![image](https://github.com/user-attachments/assets/3e17fb69-fecb-4f75-b995-0a0c1dfd6d78)

Aplikasi yang meneruskan atau menulis semua data ini ke Kafka disebut Producer.
• Producer mengirim data ke Kafka
• Producer (opsional) menerima ACK atau NACK dari Kafka
• Jika ACK (diakui) diterima, maka semuanya baik-baik saja
• Jika NACK (tidak diakui) diterima, maka Producer tahu bahwa Kafka tidak dapat menerima data karena alasan apa pun. Dalam kasus ini, Producer secara otomatis mencoba lagi untuk mengirim data.
• Banyak Producer dapat mengirim data ke Kafka secara bersamaan

Producer dapat ditulis dalam bahasa apa pun, namun Nativenya adalah Java, C/C++, Python, Go, .NET, dan JMS. Banyak juga bahasa yang berasal dari Community, dan Proxy REST untuk Bahasa apa pun yang tidak didukung.

## Broker

![image](https://github.com/user-attachments/assets/5e71c528-c7bf-441e-bd7a-6eaa93d0566a)

Kafka terdiri dari sekelompok yang disebut broker. Secara formal, Kafka adalah sebuah cluster yang terdiri dari banyak broker 

• Broker menerima data dari Producer dan menyimpannya sementara di page cache, atau secara permanen di disk setelah OS membersihkan page cache
• Broker menyimpan data agar siap untuk di-streaming consumer 

• Berapa lama data disimpan ditentukan oleh apa yang disebut **retention time** (1 minggu secara default)

## Consumer

![image](https://github.com/user-attachments/assets/4774bddd-6b79-48d4-9ab1-a1d73fdac833)

Aplikasi yang membaca data ini ke Kafka disebut Consumer.
• Seorang Consumer melakukan polling data dari Kafka
• Setiap Consumer secara berkala bertanya kepada broker klaster Kafka: "Apakah Anda memiliki lebih banyak data untuk saya?". Biasanya konsumen melakukan ini dalam lingkaran tanpa akhir.
• Banyak Consumer dapat melakukan polling data dari Kafka pada saat yang sama
• Banyak Consumer yang berbeda dapat melakukan polling data yang sama, masing-masing dengan kecepatan mereka sendiri
• Untuk memungkinkan paralelisme, Consumer diorganisasikan dalam **consumer group** yang membagi pekerjaan

Pesan masuk baru diambil secara otomatis oleh consumer dengan melacak pesan terakhir yang dibaca (Offset). Offset consumer disimpan dalam topik khusus `__consumer_offsets`.

![image](https://github.com/user-attachments/assets/11369d04-fcd1-41fb-986f-83800f92235b)

Consumer group terdiri dari 1 hingga banyak instans konsumen yang memparalelkan pekerjaan hingga jumlah instans consumer sama dengan jumlah partisi topik. Semua instans consumer dalam consumer group merupakan klon identik satu sama lain. Agar dapat ditambahkan secara transparan ke dalam consumer group, instans perlu menggunakan group.id yang sama. 

Penyeimbangan ulang partisi secara otomatis dalam grup konsumen dipicu ketika konsumen baru ditambahkan ke grup atau konsumen dihapus dari grup, misalnya untuk mengecilkan atau karena grup mogok.

Strategi penugasan partisi kepada consumer pada penyeimbangan ulang adalah:

• Range (default): Menugaskan partisi sama cocok ke Konsumen yang sama. Strategi ini berguna untuk "co-partitioning", yang khususnya berguna untuk Topik dengan pesan yang dikunci. Bayangkan kedua Topik ini menggunakan kunci yang sama - misalnya, userid.

• Range (continued): Topik A melacak hasil penelusuran untuk ID pengguna tertentu; Topik B melacak klik penelusuran untuk set ID pengguna yang sama. Dengan menggunakan ID pengguna yang sama untuk kunci di kedua Topik, pesan dengan kunci yang sama akan masuk ke Partisi bernomor yang sama di kedua Topik (dengan asumsi kedua topik memiliki jumlah Partisi yang sama) dan akan masuk ke
consumer yang sama.

• Round robin: Strategi RoundRobin jauh lebih sederhana. Partisi ditetapkan satu per satu ke Konsumen secara bergiliran hingga semua Partisi telah ditetapkan.

• Sticky: Catatan penting tentang Range dan RoundRobin bahwa keduanya tidak menjamin bahwa consumer akan mempertahankan Partisi yang sama setelah penugasan ulang. Jika aplikasi mengharuskan penugasan Partisi dipertahankan di seluruh penugasan ulang, kita dapat menggunakan strategi Sticky. Sticky adalah RoundRobin dengan pelestarian penugasan. 

• Sticky Kooperatif: Mengikuti logika StickyAssignor yang sama, tetapi memungkinkan penyeimbangan ulang kooperatif; yang berarti bahwa hanya konsumen yang mengubah penugasan yang menghentikan konsumsi selama penyeimbangan ulang, consumer lain tetap mengonsumsi.

## Zookeeper

![image](https://github.com/user-attachments/assets/fbf4cd46-b254-46f2-973c-1304a22d3144)

Broker Kafka menggunakan ZooKeeper untuk sejumlah fitur internal penting seperti

- Manajemen klaster
- Deteksi dan pemulihan kegagalan (misalnya saat broker mati)
- Untuk menyimpan Access Control Lists (ACL) yang digunakan untuk otorisasi di klaster Kafka

## Arsikektur

![image](https://github.com/user-attachments/assets/166c683a-7d89-42f4-a583-1d2d25c752a8)

• Produser di sebelah kiri menulis data ke Kafka
• Di tengah, kluster Kafka yang terdiri dari banyak broker yang menerima dan menyimpan
data
• Di sebelah kanan, consumer yang melakukan polling atau membaca data dari Kafka untuk pemrosesan streaming
• Di atas, ada kluster instans ZooKeeper yang membentuk apa yang disebut ensemble

Fitur utama Kafka adalah Producer dan Consumer dipisahkan, yaitu mereka tidak perlu mengetahui keberadaan satu sama lain, sehingga:
- Konsumen yang lambat tidak memengaruhi Producer
- Consumer yang lambat tidak akan memengaruhi Producer.
- Penambahan Consumer tidak memengaruhi Producer
- Kegagalan Consumer tidak memengaruhi Sistem upstream
- Logika internal Producer tidak pernah bergantung pada downstream consumer 
- Internal konsumen tidak bergantung pada upstream producer
- Producer dan Consumer hanya perlu menyetujui format data record yang diproduksi dan dikonsumsi.

## Topic

![image](https://github.com/user-attachments/assets/1417cb54-4184-4091-9590-148ec80af4df)

Topic adalah aliran pesan yang berhubungan di Kafka. Topic merupakan representasi logika yang mengkategorikan pesan-pesan ke dalam kelompok. Jumlah topic pada broker tidak terbatas.

![image](https://github.com/user-attachments/assets/a65df54d-5eb8-49cc-a55d-3a9acfdf2cf9)

• **Topik:** Topik mencakup semua pesan dari kategori tertentu. Misalnya, kita dapat memiliki topik "temperature_readings" yang akan berisi semua pesan yang berisi pembacaan suhu dari salah satu dari banyak stasiun pengukuran yang dimiliki perusahaan di seluruh dunia. 

• **Partisi**: Untuk memparalelkan pekerjaan dan dengan demikian meningkatkan hasil, Kafka dapat membagi satu topik menjadi banyak partisi. Pesan-pesan dari topik tersebut kemudian akan dibagi di antara partisi-partisi tersebut. Algoritme default yang digunakan untuk memutuskan ke partisi mana pesan akan dikirim menggunakan kode hash dari kunci pesan. Partisi ditangani secara keseluruhan oleh satu broker Kafka. Partisi dapat dilihat sebagai "log".

• **Segmen**: Broker menyimpan pesan saat masuk ke memori (cache halaman), kemudian secara berkala membuangnya ke file fisik. Karena datanya berpotensi tidak terbatas, broker menggunakan strategi "rolling-file". Ia membuat/mengalokasikan berkas baru dan mengisinya dengan
pesan. Ketika segmen penuh (atau waktu maksimum yang diberikan per segmen berakhir), yang berikutnya dialokasikan oleh broker. Kafka menggulirkan berkas dalam segmen untuk memudahkan mengelola retensi data dan menghapus data lama

## Log

![image](https://github.com/user-attachments/assets/99ce7421-4b55-4fd6-8dcb-a9b5fa403a12)

Log adalah struktur data yang seperti antrean elemen. Elemen baru selalu ditambahkan di akhir log dan setelah ditulis, elemen tersebut tidak pernah diubah. Dalam hal ini, seseorang berbicara tentang struktur data yang hanya ditambahkan dan ditulis sekali. Elemen yang ditambahkan ke log diurutkan secara ketat berdasarkan waktu. Elemen pertama yang ditambahkan ke log lebih lama daripada elemen kedua yang pada gilirannya lebih lama daripada elemen ketiga.

## Element Data

![image](https://github.com/user-attachments/assets/b157e87a-b898-46c0-827c-06389ea76d82)

Elemen data dalam log disebut **record** dalam dunia Kafka, bisa juga **message** atau **event**. Record dalam Kafka terdiri dari **Metadata** dan **Body**. 
• Metadata berisi offset, kompresi, magic byte, timestamp, dan kumpulan header opsional dari 0 hingga banyak pasangan key value. 
• Body terdiri dari bagian Key dan Value
• Bagian value biasanya berisi data bisnis yang relevan 
• Key secara default digunakan untuk memutuskan partisi mana rekaman ditulis. Akibatnya, semua rekaman dengan kunci yang identik masuk ke partisi yang sama. 
• Pada timestamp, creation time merupakan waktu dimana message dibuat, sedangkan ingestion time adalah waktu dimana message diterima oleh broker.

Default Strategi Partitioning: 
`hash(key) % number_of_partitions`

No Key → `Round-Robin`

![image](https://github.com/user-attachments/assets/3974ea1f-b3be-4f98-b1a6-59caab3bf76d)

Log berisi peristiwa yang dihasilkan oleh sumber data, dengan setiap peristiwa memiliki offset, kunci, dan nilai. "Log Compaction" menciptakan aliran baru yang hanya menyimpan peristiwa terakhir per kunci, berguna untuk memulihkan status setelah kegagalan sistem. Log yang dipadatkan membantu consumer hilir memulihkan status dan bermanfaat untuk layanan dalam memori, penyimpanan data persisten, dan memuat ulang cache. Pemadatan log menyimpan pembaruan terakhir untuk setiap kunci dan menyediakan snapshot nilai akhir.

## Replication

![image](https://github.com/user-attachments/assets/547bfc85-8094-492c-b342-4bafd237071e)

Kafka dapat mereplikasi partisi di sejumlah server Kafka yang dapat dikonfigurasi yang digunakan untuk toleransi kesalahan. Setiap partisi memiliki server pemimpin dan nol atau lebih server pengikut. Pemimpin menangani semua permintaan baca dan tulis untuk partisi. Dalam gambar ini, kita memiliki empat broker dan tiga partisi yang direplikasi. Faktor replikasi juga 3. Untuk setiap partisi, broker yang berbeda adalah pemimpin, untuk penggunaan sumber daya yang optimal.

## Producer di Java

![image](https://github.com/user-attachments/assets/54e2ef3f-c83e-45e4-b50e-acc2d2170efd)

Saat menulis Producer dasar di Java, kita dapat membedakan empat bagian:

• Konfigurasi: ini adalah bagian tempat kita mendefinisikan semua properti non-default dari producer kita
• Konstruktor: Di sini kita membangun objek produser menggunakan nilai konfigurasi
• Shutdown: Di bagian ini kita mendefinisikan bagaimana aplikasi akan berperilaku jika menerima sinyal `SIG_TERM` atau `SIG_KILL`
• Sending: Di bagian ini kita mendefinisikan logika yang benar-benar mengirim pesan ke topik terkait di kluster Kafka

![image](https://github.com/user-attachments/assets/3f982a9f-1734-4fd8-a4b3-44fcd44c0394)


## Consumer di .NET/C#

![image](https://github.com/user-attachments/assets/c88e0b1d-cad3-4e1a-ae8e-6c5624b111c1)

Saat menulis consumer dasar (di sini dalam C#, .NET) kita dapat membedakan bagian-bagian berikut:

• Konfigurasi: Di ​​bagian ini kita mendefinisikan nilai non-default dari berbagai parameter konfigurasi yang digunakan saat membuat objek konsumen.
• Message Callback: Dalam callback ini, yang dipicu untuk setiap pesan, kita mendefinisikan apa yang akan terjadi dengan pesan tertentu.
• Error Callbacks: Setiap kali terjadi kesalahan selama penanganan pesan atau kesalahan tak terduga, salah satu metode panggilan balik ini dipanggil. Di sini kita hanya melaporkan pengecualian tersebut ke `STDOUT`
• Subscription: Di sini konsumen berlangganan ke topik yang diinginkan
• Polling: Di sini kita mendefinisikan bagaimana consumer akan melakukan polling. Dalam contoh ini dalam loop tak berujung dengan waktu tunggu 100 ms di antara polling

## Delivery

![image](https://github.com/user-attachments/assets/8fec0296-d718-4012-8cbb-1fe810b8b310)

Pengaturan konfigurasi acks adalah jumlah pengakuan-tulis yang diterima yang diperlukan dari pemimpin partisi sebelum permintaan tulis producer dianggap selesai. Pengaturan ini mengontrol ketahanan producer yang bisa sangat kuat (all) atau tidak sama sekali (none). 

Ketahanan adalah tradeoff antara throughput dan konsistensi. 

• **Acks 0 (NONE):** Producer tidak menunggu ack apa pun dari broker Kafka sama sekali. Rekaman yang ditambahkan ke buffer soket dianggap terkirim. Tidak ada jaminan ketahanan. Offset rekaman yang dikembalikan dari metode kirim diatur ke -1
(unknown). Mungkin ada kehilangan rekaman jika pemimpin mati. Mungkin ada kasus penggunaan yang perlu memaksimalkan throughput daripada daya tahan, misalnya, agregasi log. 

• **Acks 1 (LEADER):** Pengakuan pemimpin. Ini berarti bahwa broker Kafka mengakui bahwa pemimpin partisi menulis rekaman ke log lokalnya tetapi merespons tanpa pengikut partisi mengonfirmasikan penulisan. Jika pemimpin gagal tepat setelah mengirim ack, rekaman dapat hilang karena pengikut mungkin belum mereplikasi rekaman. Kehilangan rekaman jarang terjadi tetapi mungkin, dan mungkin hanya melihat ini digunakan jika rekaman yang jarang terlewatkan tidak signifikan secara statistik, agregasi log, kumpulan data untuk pembelajaran mesin atau dasbor, dll. 

• **Acks -1 (ALL):** semua pengakuan yang berarti pemimpin mendapat konfirmasi penulisan dari set lengkap ISR sebelum mengirim ack kembali ke produsen. Ini menjamin bahwa rekaman tidak hilang selama satu ISR tetap aktif. Pengaturan ack=all ini adalah jaminan terkuat yang tersedia yang diberikan Kafka untuk ketahanan. Pengaturan ini bahkan lebih kuat dengan pengaturan broker min.insync.replicas yang menentukan jumlah minimum ISR yang harus mengakui penulisan. Sebagian besar kasus penggunaan akan menggunakan acks=all dan menetapkan min.insync.replicas > 1.

![image](https://github.com/user-attachments/assets/8d22be3c-d4e4-45eb-9265-a15830988f0f)

Kafka mendukung 3 jaminan pengiriman:

• **At most once:** Dari semua catatan yang ditulis ke Kafka, dijamin tidak akan pernah ada duplikat. Dalam keadaan buruk tertentu, beberapa catatan mungkin hilang

• **At least once:** Dari semua catatan yang ditulis ke Kafka, tidak ada yang pernah hilang. Dalam situasi buruk tertentu, mungkin ada duplikat dalam log

• **Exactly once:** Setiap catatan yang ditulis ke Kafka akan ditemukan dalam log Kafka tepat satu kali. Tidak ada situasi di mana catatan hilang atau di mana catatan diduplikasi. Untuk mencapai Exactly-Once-Semantics (EOS), langkah pertama adalah menetapkan produsen idempoten. Hal ini berguna untuk jaminan transaksional yang kuat untuk, mencegah klien memproses pesan duplikat, dan menangani kegagalan dengan baik. Kasus Penggunaan berupa melacak tampilan iklan dan memproses transaksi keuangan

## Apache Kafka Connect

![image](https://github.com/user-attachments/assets/87f67b90-ae64-4642-8180-630c6382021d)

Kafka Connect adalah kerangka kerja sumber terbuka yang merupakan bagian dari distribusi Apache Kafka, dirancang untuk mengalirkan data antara Kafka dan sistem data lainnya dengan cara yang sederhana, dapat diskalakan, dan andal. Berbeda dari API Klien atau Kafka Streams, Kafka Connect menggunakan plugin yang disebut Konektor untuk menyesuaikan perilakunya dengan titik akhir yang dipilih. Beberapa contoh kasus penggunaan Kafka Connect meliputi pengaliran seluruh basis data SQL ke Kafka, pengaliran topik Kafka ke HDFS untuk pemrosesan batch, dan pengaliran topik Kafka ke Elasticsearch untuk pengindeksan sekunder.

Contoh Konektor Sumber:
• Datagen
  ◦ bagus untuk menghasilkan data uji dan melakukan pengujian beban
  ◦ masukkan skema khusus untuk mensimulasikan kasus penggunaan Anda secara autentik
• JDBC
  ◦ bagus untuk memasukkan tabel basis data SQL ke Kafka
• Confluent Replicator
  ◦ bagus untuk memigrasikan data dari satu kluster Kafka ke kluster lainnya
• IoT Hub
  ◦ dapatkan data dari Azure IoT Hub ke Kafka
Contoh Konektor Sink:
• Apache Druid
  ◦ basis data analitik streaming yang berfungsi baik dengan Kafka
• ElasticSearch
  ◦ indeks semuanya
• AWS Simple Storage Service (S3)
  ◦ arsipkan data di S3 pada throughput tinggi
• Google Cloud Storage (GCS)
  ◦ arsipkan data di GCS pada throughput tinggi

![image](https://github.com/user-attachments/assets/fc7c6dbc-f71f-4bee-8733-0684b7dfc988)

Memberikan paralelisme dan skalabilitas dalam Kafka Connect melibatkan pemisahan beban kerja menjadi bagian-bagian yang lebih kecil, di mana pekerjaan konektor dibagi menjadi tugas-tugas yang melakukan penyalinan data. Pekerja, yang menjalankan tugas-tugas ini dalam thread terpisah, dapat mempartisi aliran input untuk paralelisme.

Dalam mode terdistribusi, Kafka Connect berjalan di setiap node pekerja, memanfaatkan protokol keanggotaan grup Kafka untuk distribusi beban. Konfigurasi konektor disimpan dalam topik Kafka untuk menjaga ketersediaan bahkan jika seorang pekerja gagal.

## Confluent Rest Proxy

![image](https://github.com/user-attachments/assets/bc984a7e-22d2-4722-9aed-5f35646b3abb)

Tiga penggunaan REST Proxy 

1. Untuk klien jarak jauh (seperti di luar pusat data termasuk melalui internet publik)
2. Untuk aplikasi klien internal yang ditulis dalam bahasa yang tidak didukung oleh pustaka klien Kafka asli (yaitu selain Java, C/C++, Python, dan Go)
3. Untuk developer atau aplikasi yang sudah ada yang REST-nya lebih produktif dan familiar daripada Kafka Producer/Consumer API.
4. Untuk melakukan tindakan administratif tanpa menggunakan protokol atau klien native Kafka.

## Confluent Schema Registry

![image](https://github.com/user-attachments/assets/cb3255d9-a80c-4a22-b684-7bd7ca1d9a44)

- Menentukan field-field yang diharapkan untuk setiap topik Kafka
- Menangani perubahan skema secara otomatis (misalnya field baru)
- Mencegah perubahan yang tidak kompatibel dengan versi sebelumnya
- Mendukung lingkungan multi-data center

Prosesnya seperti berikut:
• Serializer memanggil registri skema, untuk melihat apakah ia memiliki format untuk data yang ingin dipublikasikan oleh aplikasi.
• Jika ya, registri skema meneruskan format tersebut ke serializer aplikasi, yang menggunakannya untuk memfilter pesan yang diformat secara tidak benar. Ini menjaga alur data Anda tetap bersih.

Format yang digunakan Confluent untuk registri skemanya adalah Avro, Protobuf, dan JSON.

### AVRO

![image](https://github.com/user-attachments/assets/d40d7a36-5c5b-4e04-aeaf-bff901074b12)

Avro menyediakan serialisasi data dengan menggunakan skema yang mendeskripsikan dirinya sendiri. Avro mendukung berbagai bahasa pemrograman, termasuk Java, dan menawarkan format data terstruktur, generasi kode untuk tipe data, serta format file kontainer. Avro menyimpan data secara efisien dalam bentuk biner, dan pemeriksaan tipe dilakukan pada saat penulisan.

### Schema Evolution

![image](https://github.com/user-attachments/assets/07ed6769-5e23-4784-b5ad-34ac2ac5da3a)

Skema Avro dapat berevolusi seiring dengan pembaruan kode yang terjadi. Kita sering menginginkan kompatibilitas antar skema. Berikut ini dapat dibedakan:

◦ Backward compatibility

  ▪ Kode dengan versi baru skema dapat membaca data yang ditulis dalam skema lama
  
  ▪ Kode yang membaca data yang ditulis dengan skema akan mengasumsikan nilai default jika kolom tidak disediakan

◦ Forward compatibility

  ▪ Kode dengan versi skema sebelumnya dapat membaca data yang ditulis dalam skema baru
  
  ▪ Kode yang membaca data yang ditulis dengan skema mengabaikan kolom baru

◦ Full compatibility

  ▪ Jika kompatibilitas maju dan mundur terpenuhi

## Stream Processing

### KSqlDB

Buat aplikasi pemrosesan aliran waktu nyata yang hanya menulis SQL. Contoh:

```
 ksql> CREATE STREAM vip_actions AS
 SELECT userid, page, action
 FROM clickstream c
 LEFT JOIN users u ON c.userid = u.user_id
 WHERE u.level = ‘Platinum’;
```

ksqlDB membaca dan menulis data dari dan ke kluster Kafka Anda melalui jaringan. ksqlDB berjalan pada server khusus miliknya sendiri dan bukan sebagai bagian dari kluster Kafka

### Apache Kafka Streams

![image](https://github.com/user-attachments/assets/ebfe027b-1954-4b6c-939f-90d258379d18)

API Streams dari Apache Kafka, yang dapat diakses melalui library Java, memungkinkan pembuatan aplikasi dan layanan mikro terdistribusi yang sangat skalabel, elastis, tahan kesalahan. Fitur unik dari API Streams Kafka adalah bahwa aplikasi yang dibangun dengannya adalah aplikasi Java standar, sehingga memudahkan pengemasan, penyebaran, dan pemantauan tanpa perlu kluster pemrosesan terpisah atau infrastruktur mahal. Beberapa instansi aplikasi dapat berjalan secara independen, secara otomatis menemukan dan berkolaborasi satu sama lain. Instansi dapat ditambahkan atau dihapus secara elastis selama operasi langsung, dan jika satu instansi gagal, instansi lainnya akan secara otomatis mengambil alih pekerjaannya dan melanjutkan dari tempat yang ditinggalkannya.

![image](https://github.com/user-attachments/assets/dfae893c-4293-4973-8082-e91d5f8bf117)

Perbedaannya dari ksqldb adalah Kafka Streams lebih fleksibel dan mempunyai fitur-fitur lebih seperti filter(), map(), join(), atau aggregrate().

## Confluent Platform

![image](https://github.com/user-attachments/assets/8233ee22-4e40-4de1-af8c-0ca38145ce0d)

- **Confluent Cloud:** Penawaran yang dikelola sepenuhnya dari seluruh Platform Confluent. Penawaran ini membebaskan Anda dari pengelolaan penyebaran Anda sendiri dan memungkinkan Anda untuk berkonsentrasi pada hal-hal yang memberikan nilai bagi bisnis Anda, seperti aplikasi streaming berbasis konteks. Confluent Cloud tersedia di AWS, GCP, dan Azure.
  
- **Confluent Control Center:** GUI untuk mengelola dan memantau Apache Kafka melalui Control Center dan Health+. Alat ini memungkinkan developer dan operator untuk mengelola komponen kunci platform secara terpusat, memelihara dan mengoptimalkan kesehatan kluster, serta menggunakan peringatan cerdas untuk mengurangi waktu henti dengan mengidentifikasi masalah potensial sebelum terjadi.
  
- **Role Based Access Control (RBAC):** Memanfaatkan seperangkat peran yang telah ditentukan, di mana pengguna yang ditugaskan ke peran menerima semua hak istimewa yang terkait dengan peran tersebut. Setiap pengguna dapat memiliki beberapa peran, dan pengguna dengan hak istimewa dalam lingkup tertentu dapat membuat atau memperbarui peran untuk pengguna lain. Pengguna dengan hak istimewa ini juga memiliki kemampuan untuk memperbarui peran untuk pengguna yang sudah ada di seluruh platform, termasuk KSQL, Connect, Schema Registry, dan konektor.
