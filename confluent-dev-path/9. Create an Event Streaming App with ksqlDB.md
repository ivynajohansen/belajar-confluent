# Create an Event Streaming App with ksqlDB

## Set Up ksqlDB di Confluent Platform

Masuk ke Confluent Control Center pada lab envionment lalu pergi ke Tab **ksql**

![image](https://github.com/user-attachments/assets/bd264e3a-f367-4229-9e2e-468114785c3d)

Klik aplikasi KsqlDB yang bernama ksqldb-class

![image](https://github.com/user-attachments/assets/679d0d75-b09d-4d5c-bc43-d8646a05617b)

Buat stream bernama test yang membuat topik bernama "ksqldb.test" dengan 1 partisi. Pastikan tab editor dipilih, lalu masukkan kueri dan klik Jalankan kueri. Hasilnya akan seperti berikut:

![image](https://github.com/user-attachments/assets/6021de78-ed01-4edd-9212-fa768741b9d7)

Hapus editor dan klik Add query properties.

![image](https://github.com/user-attachments/assets/f45fdbd5-c680-4fb0-b337-2f40de0a8b43)

Set `auto.offset.reset` to `Earliest` untuk view stream dari awal.

![image](https://github.com/user-attachments/assets/4374100e-2442-4f65-8b03-22dcea0d2f20)

Masukkan query berikut dan jalankan.

```
SELECT * FROM TEST EMIT CHANGES;
```

Sekarang stream sudah dibuat dan akan muncul pada tab `Streams`:

![image](https://github.com/user-attachments/assets/e7be3825-4337-4dd9-9566-dd0e313bd36e)

## Experiment with Topics and Partitions

Masukkan query berikut di editor KsqlDB seperti sebelumnya. Kafka topic akan dibuat dengan 4 partisi.

```
CREATE TABLE users (
    name VARCHAR PRIMARY KEY,
    email VARCHAR,
    status VARCHAR,
    user_added VARCHAR
) WITH (
    KAFKA_TOPIC = 'ksqldb.users',
    PARTITIONS = 4,
    VALUE_FORMAT = 'DELIMITED',
    TIMESTAMP='user_added',
    TIMESTAMP_FORMAT = 'yyyy-MM-dd''T''HH:mm:ssX'
);
```

Hasilnya akan seperti berikut:

![image](https://github.com/user-attachments/assets/0128b1c0-cd63-4d98-9fb2-44f848a3103a)

Masukan query berikut untuk mengisi table `users` dengan dummy data:

```
insert into users (name, email, status, user_added) values ('Hall', 'hchaters0@cmu.edu', null, '2018-06-29T14:49:09Z');
insert into users (name, email, status, user_added) values ('Verge', 'vpicken1@nationalgeographic.com', 'gold', '2018-06-29T14:49:09Z');
insert into users (name, email, status, user_added) values ('Alex', 'alaurentin2@yandex.ru', 'silver', '2018-06-29T14:49:09Z');
insert into users (name, email, status, user_added) values ('Cathy', 'cdumbreck3@adobe.com', 'platinum', '2018-06-29T14:49:09Z');
insert into users (name, email, status, user_added) values ('Ruth', 'rfrankcombe4@theguardian.com', 'silver', '2018-06-29T14:49:09Z');
insert into users (name, email, status, user_added) values ('Ferne', 'fduiged5@buzzfeed.com', 'silver', '2018-06-29T14:49:09Z');
insert into users (name, email, status, user_added) values ('Joyann', 'jrigg6@youtu.be', null, '2018-06-29T14:49:09Z');
insert into users (name, email, status, user_added) values ('Ardene', 'aannear7@icio.us', 'gold', '2018-06-29T14:49:09Z');
insert into users (name, email, status, user_added) values ('Melvin', 'mdumbar8@cafepress.com', 'silver', '2018-06-29T14:49:09Z');
insert into users (name, email, status, user_added) values ('Maridel', 'mnasey9@netlog.com', 'silver', '2018-06-29T14:49:09Z');
insert into users (name, email, status, user_added) values ('Mervin', 'mbaskervillea@alibaba.com', 'silver', '2018-06-29T14:49:09Z');
insert into users (name, email, status, user_added) values ('Redd', 'rjoddensb@un.org', null, '2018-06-29T14:49:09Z');
insert into users (name, email, status, user_added) values ('Murial', 'mbokenc@csmonitor.com', 'silver', '2018-06-29T14:49:09Z');
insert into users (name, email, status, user_added) values ('Jay', 'jpalserd@nasa.gov', 'silver', '2018-06-29T14:49:09Z');
insert into users (name, email, status, user_added) values ('Sig', 'sgoodlade@mediafire.com', 'silver', '2018-06-29T14:49:09Z');
insert into users (name, email, status, user_added) values ('Nicoline', 'nhassardf@squarespace.com', 'silver', '2018-06-29T14:49:09Z');
insert into users (name, email, status, user_added) values ('Cleavland', 'cstachinig@seesaa.net', 'silver', '2018-06-29T14:49:09Z');
```

Buat stream peristiwa pembayaran yang disebut `payments` dengan jumlah partisi yang sama. Masukkan baris berikut ke dalam editor dan jalankan Kueri.

```
CREATE STREAM payments (
    name VARCHAR KEY,
    payment_id BIGINT,
    amount VARCHAR,
    payment_timestamp VARCHAR
) WITH (
    KAFKA_TOPIC = 'ksqldb.payments',
    PARTITIONS = 4,
  	VALUE_FORMAT = 'DELIMITED',
    TIMESTAMP='payment_timestamp',
    TIMESTAMP_FORMAT = 'yyyy-MM-dd''T''HH:mm:ssX'
);
```

Isi lagi dengan dummy data:

```
insert into payments (name, payment_id, amount, payment_timestamp) values ('Alex', 1, '$41.25', '2020-06-05T04:14:00Z');
insert into payments (name, payment_id, amount, payment_timestamp) values ('Verge', 2, '$59.83', '2020-06-29T14:49:09Z');
insert into payments (name, payment_id, amount, payment_timestamp) values ('Verge', 3, '$76.31', '2020-06-05T16:27:51Z');
insert into payments (name, payment_id, amount, payment_timestamp) values ('Hall', 4, '$8.28', '2020-06-05T12:21:16Z');
insert into payments (name, payment_id, amount, payment_timestamp) values ('Hall', 5, '$58.53', '2020-06-25T11:26:54Z');
insert into payments (name, payment_id, amount, payment_timestamp) values ('Cathy', 6, '$68.69', '2020-06-26T17:13:07Z');
insert into payments (name, payment_id, amount, payment_timestamp) values ('Hall', 7, '$82.37', '2020-06-29T10:25:34Z');
insert into payments (name, payment_id, amount, payment_timestamp) values ('Cathy', 8, '$12.23', '2020-06-13T02:34:14Z');
insert into payments (name, payment_id, amount, payment_timestamp) values ('Verge', 9, '$67.47', '2020-06-28T22:18:13Z');
insert into payments (name, payment_id, amount, payment_timestamp) values ('Cathy', 10, '$95.84', '2020-07-05T20:16:30Z');
insert into payments (name, payment_id, amount, payment_timestamp) values ('Cathy', 11, '$42.42', '2020-06-27T08:31:46Z');
insert into payments (name, payment_id, amount, payment_timestamp) values ('Cathy', 12, '$52.98', '2020-07-02T20:18:34Z');
insert into payments (name, payment_id, amount, payment_timestamp) values ('Alex', 13, '$74.58', '2020-06-23T08:20:18Z');
insert into payments (name, payment_id, amount, payment_timestamp) values ('Hall', 14, '$14.85', '2020-06-27T20:52:56Z');
insert into payments (name, payment_id, amount, payment_timestamp) values ('Alex', 15, '$20.21', '2020-06-02T14:41:22Z');
insert into payments (name, payment_id, amount, payment_timestamp) values ('Hall', 16, '$21.98', '2020-06-01T15:42:19Z');
insert into payments (name, payment_id, amount, payment_timestamp) values ('Verge', 17, '$30.38', '2020-06-23T21:56:11Z');
insert into payments (name, payment_id, amount, payment_timestamp) values ('Cathy', 18, '$69.63', '2020-07-05T22:25:43Z');
insert into payments (name, payment_id, amount, payment_timestamp) values ('Alex', 19, '$69.97', '2020-06-23T14:52:02Z');
insert into payments (name, payment_id, amount, payment_timestamp) values ('Cathy', 20, '$92.86', '2020-06-13T22:10:34Z');
```

Di bawah editor, klik `Add query properties` dan pastikan `auto.offset.reset` diatur ke `Earliest` sehingga kueri berikutnya akan membaca stream dari awal, bukan akhir.

Gabungkan stream pembayaran dengan tabel users untuk menampilkan semua pembayaran oleh pengguna dengan status hadiah "platinum".

```
SELECT p.name, p.amount, u.status
    FROM payments p
    LEFT JOIN users u
    ON p.name = u.name
    WHERE u.status = 'platinum'
EMIT CHANGES;
```

Hasilnya akan seperti ini:

![image](https://github.com/user-attachments/assets/539b1394-29e6-4802-975a-d6d82bb9d61a)

Partisi diperlukan di Kafka demi alasan performa, tetapi apakah partisi penting saat menggunakan ksqlDB? Sekarang coba JOIN yang sama seperti sebelumnya, tetapi kali ini stream pembayaran dan tabel pengguna akan memiliki jumlah partisi yang berbeda.

Buat stream bernama `payment_bad` dengan cara yang sama seperti membuat aliran payment asli kecuali dengan 6 partisi, bukan 4.

```
CREATE STREAM payments_bad (
    name VARCHAR KEY,
    payment_id BIGINT,
    amount VARCHAR,
    payment_timestamp VARCHAR
) WITH (
    KAFKA_TOPIC = 'ksqldb.payments.bad',
    PARTITIONS = 6,
    VALUE_FORMAT = 'DELIMITED',
    TIMESTAMP='payment_timestamp',
    TIMESTAMP_FORMAT = 'yyyy-MM-dd''T''HH:mm:ssX'
);
```

Isi dengan dummy data:

```
insert into payments_bad (name, payment_id, amount, payment_timestamp) values ('Alex', 1, '$41.25', '2020-06-05T04:14:00Z');
insert into payments_bad (name, payment_id, amount, payment_timestamp) values ('Verge', 2, '$59.83', '2020-06-29T14:49:09Z');
insert into payments_bad (name, payment_id, amount, payment_timestamp) values ('Verge', 3, '$76.31', '2020-06-05T16:27:51Z');
insert into payments_bad (name, payment_id, amount, payment_timestamp) values ('Hall', 4, '$8.28', '2020-06-05T12:21:16Z');
insert into payments_bad (name, payment_id, amount, payment_timestamp) values ('Hall', 5, '$58.53', '2020-06-25T11:26:54Z');
insert into payments_bad (name, payment_id, amount, payment_timestamp) values ('Cathy', 6, '$68.69', '2020-06-26T17:13:07Z');
insert into payments_bad (name, payment_id, amount, payment_timestamp) values ('Hall', 7, '$82.37', '2020-06-29T10:25:34Z');
insert into payments_bad (name, payment_id, amount, payment_timestamp) values ('Cathy', 8, '$12.23', '2020-06-13T02:34:14Z');
insert into payments_bad (name, payment_id, amount, payment_timestamp) values ('Verge', 9, '$67.47', '2020-06-28T22:18:13Z');
insert into payments_bad (name, payment_id, amount, payment_timestamp) values ('Cathy', 10, '$95.84', '2020-07-05T20:16:30Z');
insert into payments_bad (name, payment_id, amount, payment_timestamp) values ('Cathy', 11, '$42.42', '2020-06-27T08:31:46Z');
insert into payments_bad (name, payment_id, amount, payment_timestamp) values ('Cathy', 12, '$52.98', '2020-07-02T20:18:34Z');
insert into payments_bad (name, payment_id, amount, payment_timestamp) values ('Alex', 13, '$74.58', '2020-06-23T08:20:18Z');
insert into payments_bad (name, payment_id, amount, payment_timestamp) values ('Hall', 14, '$14.85', '2020-06-27T20:52:56Z');
insert into payments_bad (name, payment_id, amount, payment_timestamp) values ('Alex', 15, '$20.21', '2020-06-02T14:41:22Z');
insert into payments_bad (name, payment_id, amount, payment_timestamp) values ('Hall', 16, '$21.98', '2020-06-01T15:42:19Z');
insert into payments_bad (name, payment_id, amount, payment_timestamp) values ('Verge', 17, '$30.38', '2020-06-23T21:56:11Z');
insert into payments_bad (name, payment_id, amount, payment_timestamp) values ('Cathy', 18, '$69.63', '2020-07-05T22:25:43Z');
insert into payments_bad (name, payment_id, amount, payment_timestamp) values ('Alex', 19, '$69.97', '2020-06-23T14:52:02Z');
insert into payments_bad (name, payment_id, amount, payment_timestamp) values ('Cathy', 20, '$92.86', '2020-06-13T22:10:34Z');
```

Coba untuk melakukan penggabungan yang sama seperti sebelumnya. Sekali lagi, pastikan bahwa properti kueri auto.offset.reset diatur ke Earliest sehingga kueri akan membaca dari awal aliran.

```
SELECT p.name, p.amount, u.status
    FROM payments_bad p
    LEFT JOIN users u
    ON p.name = u.name
    WHERE u.status = 'platinum'
EMIT CHANGES;
```

![image](https://github.com/user-attachments/assets/09c18fc8-f0da-45cb-a67d-4e28bd2077a5)

## Capture Data with Kafka Connect

Navigasi ke Topik di Confluent Control Center. Pilih **+ Create topic** dan buat topik bernama ksqldb.stockapp.trades dengan 4 partisi dan pengaturan default. Pilih **+ Create topic** dan buat topik bernama ksqldb.stockapp.users dengan 4 partisi dan pengaturan default.

![image](https://github.com/user-attachments/assets/113c05e1-c5a5-490b-8ee0-85e524835b13)

Di KsqlDB editor, buat source connector untuk mengisi dummy data stock trades:

```
CREATE SOURCE CONNECTOR `stockapp.trades` WITH (
    'connector.class' = 'io.confluent.kafka.connect.datagen.DatagenConnector',
    'kafka.topic' = 'ksqldb.stockapp.trades',
    'quickstart' = 'Stock_Trades',
    'max.interval' = 3000,
    'iterations' = 1000,
    'tasks.max' = '1'
);
```

![image](https://github.com/user-attachments/assets/0c257a8b-02a5-4f05-ae61-840bc8f595da)

Buat source connector untuk mengisi dummy data stock users:

```
CREATE SOURCE CONNECTOR `stockapp.users` WITH (
    'connector.class' = 'io.confluent.kafka.connect.datagen.DatagenConnector',
    'kafka.topic' = 'ksqldb.stockapp.users',
    'quickstart' = 'Users_',
    'max.interval' = 3000,
    'iterations' = 1000,
    'tasks.max' = '1'
);
```

Buat stream bernama `stockapp_trades` dari topic `ksqldb.stockapp.trades`:

```
CREATE STREAM stockapp_trades WITH (
    KAFKA_TOPIC = 'ksqldb.stockapp.trades',
    VALUE_FORMAT = 'AVRO'
);
```

Query stream stock trades untuk melihat stock trades yang mengalir secara real time. Hentikan query setelah 30 detik atau lebih

```
SELECT * FROM stockapp_trades EMIT CHANGES;
```

Hasil setelah stop query:

![image](https://github.com/user-attachments/assets/422b80a6-25c5-41fd-a0ca-a4e129ee11b8)

Buat tabel bernama stockapp_users dari topik ksqldb.stockapp.users. Ini adalah tabel, bukan stream karena biasanya hanya peduli dengan informasi profil terbaru untuk setiap pengguna.

```
CREATE TABLE stockapp_users (
    userid STRING PRIMARY KEY,
    registertime BIGINT,
    regionid STRING,
    gender STRING,
    interests ARRAY<STRING>,
    contactinfo MAP<STRING, STRING>
) WITH (
    KAFKA_TOPIC = 'ksqldb.stockapp.users',
    VALUE_FORMAT = 'AVRO'
);
```

Query stream stock trades untuk melihat stock user yang mengalir secara real time. Hentikan query setelah 30 detik atau lebih.

```
SELECT * FROM stockapp_users EMIT CHANGES;
```

![image](https://github.com/user-attachments/assets/74c1bc6f-e50f-429a-a80c-4ae667a0a05a)

## Transform Data

Buat stream baru bernama `stockapp_trades_transformed` yang akan menggantikan semua huruf di `account` menjadi `*`. Stream ini hanya memilih pesan yang atribut "symbol"-nya diakhiri dengan "T". Kolom-kolom yang ditampilkan berupa dollar_amount, account_masked, symbol, dan userid.

```
CREATE STREAM stockapp_trades_transformed AS
    SELECT
        CAST(price AS DECIMAL(7,2)) * quantity / 100 AS dollar_amount,
        MASK(account, '*', '*', NULL, NULL) AS account_masked,
        symbol,
        userid
    FROM stockapp_trades
    WHERE symbol LIKE '%T'
    EMIT CHANGES;
```

Run query dan hentikan setelah 30 detik.

```
SELECT * FROM stockapp_trades_transformed EMIT CHANGES;
```

Hasilnya akan seperti berikut:

![image](https://github.com/user-attachments/assets/6b716a1d-c193-4913-8a32-f81a0bc69b0a)

## Join Data

Buat stream `stockapp_transformed_enriched` dengan menggabungkan stream stockapp_trades_transformed dengan stockapp_users. Stream yang diperkaya harus menyertakan kolom berikut:
- userid
- dolar_amount
- regionid
- interests
- contactinfo

```
CREATE STREAM stockapp_trades_transformed_enriched AS
    SELECT s.userid, s.dollar_amount, s.symbol,
           u.regionid, u.interests, u.contactinfo
    FROM stockapp_trades_transformed s
    LEFT JOIN stockapp_users u
        ON s.userid = u.userid
    EMIT CHANGES;
```

Run query berikut dan hentikan setelah beberapa detik:

```
SELECT * FROM stockapp_trades_transformed_enriched EMIT CHANGES;
```

Hasilnya akan seperti berikut. Data sudah digabungkan:

![image](https://github.com/user-attachments/assets/216564b1-85b0-4e9e-bb59-87a663dbf83c)

![image](https://github.com/user-attachments/assets/4f2120d7-6d03-45d9-b501-42518d70ae5a)

## Buat Materialized Views

Buat table `stockapp_dollars_by_zip_5_min` dengan query berikut. Kueri ini membuat tabel yang menjumlahkan jumlah dolar dari stock trades di setiap kode pos. Pengumpulan ini dilakukan dalam rentang waktu 5 menit, sehingga kode pos 94301 mungkin memiliki jumlah dolar total tertinggi dalam satu rentang waktu, dan kode pos 95112 mungkin memiliki total tertinggi di rentang waktu berikutnya.

Sebelumnya, stream input sudah difilter untuk stock trades yang simbolnya diakhiri dengan huruf "T". Stream input juga diperkaya dengan data contactinfo dari tabel stockapp_users, yang memungkinkan agregasi ini untuk dikelompokkan berdasarkan kode pos.

```
CREATE TABLE stockapp_dollars_by_zip_5_min AS
    SELECT
        contactinfo['zipcode'] AS zipcode,
        SUM(dollar_amount) AS total_dollars
    FROM stockapp_trades_transformed_enriched
    WINDOW TUMBLING (SIZE 5 MINUTES)
    GROUP BY contactinfo['zipcode']
    EMIT CHANGES;
```

Jalankan query ini selama minimal 5 menit untuk melihat perubahan window:

```
SELECT * FROM stockapp_dollars_by_zip_5_min EMIT CHANGES;
```

Sebelum 5 menit:

![image](https://github.com/user-attachments/assets/96538621-3820-4a84-a385-49a018393bda)

Setelah 5 menit:


