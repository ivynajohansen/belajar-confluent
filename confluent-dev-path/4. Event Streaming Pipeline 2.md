# Membangun Event Streaming Pipeline
## Aplikasi Apache Kafka Streams

Kafka Streams adalah library klien Java untuk membangun aplikasi dan layanan mikro, di mana data input dan output disimpan dalam cluster Kafka.

![image](https://github.com/user-attachments/assets/ec22f394-b72d-4c82-8f0b-f0ff25a05f8d)

Sebuah stream mencakup seluruh masa lalu, masa kini, dan masa depan.

Dalam gambar di atas, kita melihat aliran event tersebut di sisi kiri. Setiap event merupakan debit atau kredit ke rekening bank. Seseorang bisa melihat seluruh riwayat dari stream, namun, jika hanya ingin melihat status saldo pada waktu tertentu, bisa gunakan table.

![image](https://github.com/user-attachments/assets/fa0c38c3-af75-4bcf-9de2-8c96a2f81dfb)

• Source Processor: Sebuah source processor adalah jenis khusus dari stream processor yang tidak memiliki prosesor hulu. Ia menghasilkan aliran input ke topologinya dari satu atau beberapa topik Kafka dengan menggunakan rekaman dari topik-topik ini dan meneruskannya ke prosesor hilirnya. 

• Sink Processor: Sebuah sink processor adalah jenis khusus dari stream processor yang tidak memiliki prosesor hilir. Ia mengirimkan record yang diterima dari prosesor hulunya ke topik Kafka tertentu.

![image](https://github.com/user-attachments/assets/a5287bf4-ebbe-4de4-acde-99558e3b2390)

Diagram ini menunjukkan aplikasi Kafka Streams yang memiliki beberapa thread stream, masing-masing dengan beberapa tugas. Thread dapat berjalan paralel di satu sistem atau didistribusikan untuk skalabilitas. Setiap thread memiliki konsumen dan produsen, serta tugas yang menyimpan status lokal di RocksDB, yang dipersistenkan ke topik changelog Kafka.

Windows:
- Membagi stream menjadi "time bucket"
- Tumbling, Hopping, dan Session Windows

![image](https://github.com/user-attachments/assets/2e174f1f-b447-4cf9-bdc9-1349b6e7fd2a)

Aggregations:
- Mengakumulasi sejumlah value saat rekaman baru masuk
- Biasanya berjendela
- Contoh: sum, count, max, min

Joins:
- Gabungkan stream/tabel yang berbeda bersama-sama pada sebuah kunci
- Contoh: Gabungkan aliran lokasi pengemudi dengan tabel profil pengemudi pada kunci ID pengemudi untuk membuat stream yang diperkaya dengan info lokasi dan profil
- Dapat dijendela dengan "sliding window"

### Contoh Kafka Streams (Konfigurasi)

```
public class SimpleStreamsExample {
   public static void main(String[] args) throws Exception {
      Properties config = new Properties();
      // Give the Streams application a unique name. The name must be unique in the Kafka cluster
      config.put(StreamsConfig.APPLICATION_ID_CONFIG, "simple-streams-example");
      config.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "broker-1:9092,broker-2:9092");
      config.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
      // Specify default (de)serializers for record keys and for record values.
      config.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.ByteArray().getClass());
      config.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());
  }
}
```

- Baris 8 menunjukkan konfigurasi khusus untuk consumer aplikasi Kafka Streams. Ini memungkinkan untuk melakukan hal-hal seperti menyetel pengaturan batch produser di aplikasi Kafka Streams.
- Baris 10 dan 12 menunjukkan kunci dan nilai SerDes. Dalam contoh ini, tipe data akan sama untuk pesan masuk dan keluar sehingga SerDes tidak perlu ditimpa oleh kode.

### Contoh Kafka Streams (Topology)

```
     StreamsBuilder builder = new StreamsBuilder();
  
     // Construct a KStream from the input Topic "TextLinesTopic"
     KStream<byte[], String> textLines = builder.stream("TextLinesTopic");
     // Convert to upper case
     KStream<byte[], String> uppercasedWithMapValues =
     textLines.mapValues(value -> value.toUpperCase());
     // Write the results to a new Kafka Topic called "UppercasedTextLinesTopic".
     uppercasedWithMapValues.to("UppercasedTextLinesTopic");
     // Run the Streams application via `start()`
     streams.start();
     KafkaStreams streams = new KafkaStreams(builder.build(), config);
     // Stop the application gracefully
     Runtime.getRuntime().addShutdownHook(new Thread(streams::close));
  }
}
```

Objek KStream textLines dibuat dengan menggunakan topik TextLinesTopic. textLines diubah oleh mapValues ​​untuk mengubah semua karakter dalam pesan menjadi huruf kapital, sehingga menghasilkan objek KStream yang disebut uppercasedWithMapValues. KStream yang dihasilkan kemudian diproduksi ke topik UppercasedTextLinesTopic.

## Kafka Connect

![image](https://github.com/user-attachments/assets/5e442ef0-218f-4b5e-9f76-177faff8c676)

Kafka Connect adalah kerangka kerja untuk streaming data antara Apache Kafka dan sistem data lainnya. 

- Connector: pekerjaan logis yang menyalin data antara Kafka dan sistem lain

- Source Connector: membaca data dari sistem data eksternal ke Kafka 

   - Secara internal, konektor sumber menggunakan Kafka Producer
  
- Sink Connector: menulis data Kafka ke sistem data eksternal

   - Secara internal, konektor sink menggunakan Kafka Consumer Group

![image](https://github.com/user-attachments/assets/ed3cbc0f-a882-4b2d-8b81-4a895ffd6528)

Contoh kasus penggunaan untuk Kafka Connect meliputi:
- Mengalirkan seluruh basis data SQL ke Kafka
   - Bulk - memuat seluruh tabel
   - Change Data Capture (CDC) - memuat perubahan tabel saat terjadi
- Mengimpor file CSV yang dihasilkan oleh aplikasi lama ke Kafka
- Mengalirkan Topik Kafka ke Hadoop File System (HDFS) untuk pemrosesan batch
- Mengalirkan Topik Kafka ke Elasticsearch untuk pengindeksan sekunder
- Mengarsipkan data lama dalam penyimpanan objek berbiaya rendah
   - misalnya Amazon Simple Storage Service (S3)

Gunakan klien confluent-hub yang disertakan dengan Confluent Platform untuk menginstall salah satu connector dari Confluent Hub:

```
confluent-hub install debezium/debezium-connector-mysql:latest
```

![image](https://github.com/user-attachments/assets/3d4ced49-ad3c-474f-80f1-baf0281a924e)

- Membagi beban kerja menjadi bagian-bagian yang lebih kecil memberikan paralelisme dan skalabilitas
- Pekerjaan konektor dipecah menjadi task-task yang melakukan penyalinan data yang sebenarnya
- Worker adalah proses yang menjalankan satu atau lebih tugas, masing-masing dalam utas yang berbeda


![image](https://github.com/user-attachments/assets/48e9e1c1-f34f-4f3a-acdb-a9f33b61a5c9)

Kafka Connect, saat dijalankan dalam mode terdistribusi, secara otomatis dan transparan melakukan failover. Di sini ada 4 pekerja dengan 3 konektor A, B, C. Konektor A memiliki 4 tugas yang berjalan pada pekerja 1 dan 2 sementara konektor B dan C masing-masing memiliki satu tugas yang berjalan pada pekerja 3 dan 4.

![image](https://github.com/user-attachments/assets/8434eb24-851a-434e-829f-312b20f71c9c)

Jika Worker 4 gagal, task akan di-rebalance ke worker-worker yang tersisa. Dalam kasus ini, job konektor B telah dipindahkan ke pekerja 2 sementara job konektor C dan task C1 telah dipindahkan ke pekerja 3.

Berikut adalah konfigurasi penting untuk worker:

| Parameter          | Deskripsi                                                                                          |
|--------------------|---------------------------------------------------------------------------------------------------|
| bootstrap.servers  | Sebuah daftar pasangan host/port yang digunakan untuk membangun koneksi awal ke cluster Kafka.    |
| key.converter       | Kelas konverter untuk kunci.                                                                      |
| value.converter     | Kelas konverter untuk nilai.                                                                       |
| group.id           | Sebuah string unik yang mengidentifikasi grup cluster Kafka Connect tempat pekerja mode distribusi berada. |

Berikut adalah endpoint REST:

| Metode | Path                            | Deskripsi                                                    |
|--------|-----------------------------------|-------------------------------------------------------------|
| GET    | /connectors                       | Mendapatkan daftar konektor aktif                          |
| POST   | /connectors                       | Membuat konektor baru                                       |
| GET    | /connectors/(string: name)/config | Mendapatkan informasi konfigurasi untuk sebuah konektor     |
| PUT    | /connectors/(string: name)/config | Membuat konektor baru atau memperbarui konfigurasi konektor yang sudah ada |

Konfigurasi Connector:

```
{
  "name": "Driver-Connector",
  "config": {
    "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
    "connection.url": "jdbc:postgresql://postgres:5432/postgres",
    "connection.user": "postgres",
    "table.whitelist": "driver",
    "topic.prefix": "my-postgres-",
    "mode": "timestamp+incrementing",
    "incrementing.column.name": "id",
    "timestamp.column.name": "timestamp",
    "table.types": "TABLE",
    "numeric.mapping": "best_fit"
  }
}
```

Konektor dapat ditambahkan, dimodifikasi, dan dihapus melalui REST API pada port 8083. Dalam mode terdistribusi, konfigurasi hanya dapat dilakukan melalui REST API. Dalam mode standalone, konfigurasi juga dapat dilakukan melalui REST API. Namun, biasanya konfigurasi dilakukan melalui file properti.

![image](https://github.com/user-attachments/assets/d85e4116-8e5d-45ed-b490-0c3f15279d34)

Transformasi pesan tunggal (SMT) adalah bagian penting dari jalur pemrosesan Kafka Connect, terutama untuk konektor sumber. Berikut adalah cara kerjanya dalam proses:

- Konektor Sumber: Membaca data dari sistem eksternal dan menyimpannya di Kafka.
- Terjemahan Data: Tugas dari konektor membaca data dalam format aslinya dan mengubahnya menjadi API data generik Kafka Connect.
- Transformasi: Setelah berada dalam format generik, SMT dapat diterapkan, memungkinkan transformasi apa pun tanpa memandang format data asli.
- Penyelesaian: Data yang telah ditransformasi tetap dalam format generik dan kemudian diserialisasi menjadi Avro atau JSON oleh konverter sebelum dikirim ke Kafka.
- Dapat Digunakan Kembali: Proses ini memungkinkan transformasi umum yang dapat diterapkan pada konektor mana pun, mirip dengan cara format serialisasi dapat digunakan kembali di seluruh konektor.

| **Operasi**      | **Deskripsi**                                                                                      |
|------------------|---------------------------------------------------------------------------------------------------|
| InsertField      | Menyisipkan sebuah field menggunakan atribut dari metadata pesan atau dari nilai statis yang dikonfigurasi. |
| ReplaceField     | Mengganti nama field, atau menerapkan daftar untuk memfilter.                                     |
| MaskField        | Mengganti field dengan nilai null yang valid untuk tipe tersebut (0, string kosong, dll).         |
| ValueToKey       | Mengganti kunci dengan kunci baru yang dibentuk dari subset field dalam payload nilai.           |
| HoistField       | Mengemas seluruh event sebagai satu field di dalam Struct atau Map.                              |
| ExtractField     | Mengekstrak field tertentu dari Struct dan Map dan hanya menyertakan field ini dalam hasil.      |


![image](https://github.com/user-attachments/assets/d160df3f-0dbc-4bcb-b87b-b0b7ff46fe22)

- Konverter menyediakan format data yang ditulis atau dibaca dari Kafka (seperti Serializer)
- Konverter dipisahkan dari Konektor sehingga dapat digunakan kembali
   - Memungkinkan konektor apa pun bekerja dengan format serialisasi apa pun
 
Konverter berlaku untuk kunci dan nilai pesan
- Konverter kunci dan nilai dapat diatur secara independen
   - key.converter
   - value.converter
- Format data yang telah ditetapkan sebelumnya untuk Konverter
   - Avro: AvroConverter
   - Byte Array: ByteArrayConverter
   - JSON: JsonConverter
   - Skema JSON: JsonSchemaConverter
   - Protobuf: ProtobufConverter
   - String: StringConverter
 
Contoh:

```
key.converter=io.confluent.connect.avro.AvroConverter
key.converter.schema.registry.url=http://schemaregistry1:8081
value.converter=io.confluent.connect.avro.AvroConverter
value.converter.schema.registry.url=http://schemaregistry1:8081
```

## KSqlDB

![image](https://github.com/user-attachments/assets/7da6e7a7-3287-4f81-b292-321037823d6a)

ksqlDB menyederhanakan kompleksitas dalam membangun aplikasi pemrosesan streaming, memungkinkan pengembang dan analis untuk dengan cepat menciptakan sistem real-time dengan overhead yang minimal. Ini menggabungkan kemampuan pemrosesan aliran real-time dengan pengalaman basis data yang mudah digunakan, menggunakan sintaks SQL yang familiar dan ringan. Karena didukung secara native oleh Apache Kafka®, ksqlDB secara efektif memanfaatkan platform streaming peristiwa yang kuat.

Contoh:

```
CREATE TABLE possible_fraud AS
   SELECT card_number, count(*)
   FROM authorization_attempts
   WINDOW HOPPING (SIZE 5 SECONDS, ADVANCE BY 1 SECOND)
   GROUP BY card_number
   HAVING count(*) > 3;
```

Contoh ini menunjukkan cara membuat tabel baru bernama "possible_fraud" dengan memantau frekuensi kemunculan bidang yang disebut "card_number" dalam aliran bernama "authorization_attempts." Analisis dilakukan dalam jendela waktu bergulir selama 5 detik. Jika "card_number" terdeteksi lebih dari tiga kali dalam jendela ini, sebuah pesan dihasilkan, menggunakan "card_number" sebagai kunci dan jumlah kemunculan sebagai nilai, yang kemudian disimpan dalam tabel "possible_fraud."

| Perintah | Deskripsi |
|----------|-----------|
| CREATE   | Membuat stream, tabel, konektor Kafka Connect, dan "tipe" kustom |
| SELECT   | Membuat kueri push atau pull |
| SHOW     | Menampilkan informasi tentang aliran, tabel, tipe, properti, fungsi, kueri topik, dll. |
| DESCRIBE | Menampilkan nama dan tipe kolom, metrik untuk aliran dan tabel, atau memeriksa status konektor |
| DROP     | Menghapus stream, tabel, konektor, atau tipe yang sudah ada |

### Query Persisten vs. Non-persisten

- Kueri non-persisten
   - Hasil dari kueri non-persisten tidak akan disimpan ke dalam Topik Kafka dan hanya akan dicetak di konsol
- Kueri persisten
   - Hasil dari kueri persisten akan disimpan ke dalam Topik Kafka

| Perintah                          | Deskripsi    |
|-----------------------------------|--------------|
| SELECT                            | Non-persistent |
| CREATE STREAM AS SELECT           | Persistent   |
| CREATE TABLE AS SELECT            | Persistent   |

### Pull vs Push Query

Pull Query:
- Mencari nilai saat ini dari tabel materialisasi.
- Mengembalikan respons tunggal dalam permintaan HTTP.

```
SELECT * FROM viewsPerUserSession
   WHERE ROWKEY = 'cool-kafka-user-5'
   AND 1570051876000 <= WINDOWSTART
   AND WINDOWSTART <= 1570123744000;
```
Push Query:

- Subscribe ke stream
- Dilambangkan dengan `EMIT CHANGES`
• Respons HTTP yang dipotong-potong dengan panjang yang tidak terbatas

```
SELECT * FROM user_table
   EMIT CHANGES;
```

### Contoh-contoh Pembuatan Stream dan Tabel

Buat aliran dari topik Kafka `my-pageviews-topic`, yang memiliki muatan JSON

```
CREATE STREAM pageviews (viewtime BIGINT, user_id VARCHAR, page_id VARCHAR)
   WITH (VALUE_FORMAT = 'JSON', KAFKA_TOPIC = 'my-pageviews-topic');
```

Buat tabel dari topik Kafka `my-users-topic`, yang memiliki muatan JSON

```
CREATE TABLE users (usertimestamp BIGINT, user_id VARCHAR, gender VARCHAR, region_id VARCHAR)
   WITH (VALUE_FORMAT = 'JSON', KAFKA_TOPIC = 'my-users-topic', KEY = 'user_id');
```

#### Persistent Query dengan Join
Perkaya stream pageviews dengan menggabungkannya dengan tabel users pada user_id:

```
CREATE STREAM pageviews_enriched AS
 SELECT pv.user_id AS user_id,
 pv.viewtime,
 pv.pageid,
 u.gender,
 u.region
 FROM pageviews pv
 LEFT JOIN users u
 ON pv.user_id = u.user_id
 EMIT CHANGES;
```

#### Persistent Query dengan Windowing

Gabungkan stream tampilan pageviews yang diperkaya berdasarkan jenis kelamin dan wilayah:

```
CREATE TABLE pageviews_count_by_region AS
 SELECT gender, regionid, COUNT(*) AS total
 FROM pageviews_enriched
 WINDOW TUMBLING (SIZE 30 SECONDS)
 GROUP BY gender, regionid
 HAVING COUNT(*) > 1
 EMIT CHANGES;
```

Di sini, kami membuat tabel berjendela pageviews_count_by_region yang menentukan jendela putar 30 detik, dan di dalam setiap jendela, mengelompokkan berdasarkan jenis kelamin lalu berdasarkan wilayah, dan menghitung hasilnya.

Untuk melakukan pull query terhadap table tersebut:

```
SELECT * FROM pageviews_count_by_region
   WHERE ROWKEY='female|+|Asia'
      AND WINDOWSTART >= 1557183890000
      AND WINDOWSTART <= 1557184000000;
```

Outputnya akan seperti berikut:

![image](https://github.com/user-attachments/assets/ae80ce9a-da01-40c1-a99a-58b9ae79fa98)

## Partition

Arsitek Solusi di Confluent menyarankan bahwa 30 partisi umumnya cukup untuk menskalakan sebagian besar aplikasi. Angka 30 dapat dibagi oleh banyak bilangan (2, 3, 5, 6, 10, 15), memungkinkan fleksibilitas dalam menskalakan jumlah konsumen. Beban tambahan untuk partisi minimal hingga melebihi 4000 per broker atau 200.000 per cluster. Disarankan untuk "over-partition" saat membuat topik.

Jumlah partisi yang disarankan: max(t/p, t/c)
- t: target throughput
- p: Throughput produsen per partisi
- c: Throughput konsumen per partisi

 Variasikan Producer parameters:
- Replication factor
- Message size
- In flight requests per connection (max.in.flight.requests.per.connection)
- Batch size (batch.size)
- Batch wait time (linger.ms)
  
- Vary Consumer parameters:
- Fetch size (fetch.min.bytes)
- Fetch wait time (fetch.max.wait.ms)

### Meningkatkan jumlah partisi

Beberapa aplikasi dapat menerima begitu saja bahwa kunci akan mengubah partisi dan karenanya akan melanjutkan penggunaannya.

![image](https://github.com/user-attachments/assets/4deef42d-4ca4-44af-953a-72a9b4a40ad3)

Pada gambar ini, ikon dengan bentuk yang sama mewakili record dengan kunci yang sama. Topik awalnya memiliki 2 partisi dan 3 konsumer, yang membuat satu konsumer tidak aktif. Ketika kita menambah jumlah partisi menjadi 3, grup konsumer sekarang menggunakan ketiga konsumer tersebut. Namun, kunci sekarang dikirimkan ke partisi yang berbeda dari sebelumnya. Kotak sekarang dikirimkan ke P0, bintang tetap berada di P1, dan lingkaran sekarang menuju P2.

Terkadang masuk akal untuk menerapkan partitioner khusus. Misalnya, perusahaan perdagangan saham dapat memperdagangkan banyak saham dan menetapkan kunci untuk peristiwa perdagangan sebagai ID saham. Jika perusahaan memperdagangkan 1000 saham, tetapi hanya 5 saham yang mencakup 90% dari perdagangan mereka, maka masuk akal untuk menulis partitioner yang menetapkan setiap "hot keys" tersebut partisi khusus.

```
public class MyPartitioner implements Partitioner {
   public void configure(Map<String, ?> configs) {}       
   public void close() {}
   public void onNewBatch() {}
   public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
      int numPartitions = cluster.partitionsForTopic(topic).size();
      if ((keyBytes == null) || (!(key instanceof String)))
      throw new InvalidRecordException("Record did not have a string Key");
```

- Untuk membuat Partitioner kustom, implementasikan interface `Partitioner`
   - Interface ini mencakup `configure()`, `close()`, `onNewBatch()`, dan metode `partition()`, meskipun sering kali hanya akan mengimplementasikan partition()
- `partition()` diberikan Topik, kunci, kunci serial, nilai, nilai serial, dan metadata cluster
   - Ini harus mengembalikan nomor Partisi yang harus dikirimi pesan khusus ini (berbasis 0)
- `onNewBatch()` ditambahkan di AK 2.4 untuk memungkinkan partisi pesan "yang sadar batch" dengan kunci null. Ini memungkinkan partisi baru dipilih saat batch terisi dan batch baru dibuka. Untuk informasi lebih lanjut, lihat KIP-480

```
      if (((String) key).equals("OurBigKey"))
         return 0; // This key will always go to Partition 0
      // Other records will go to remaining partitions using a hashing function
      return (Math.abs(Utils.murmur2(keyBytes)) % (numPartitions - 1)) + 1;
   }
}
```

Partisioner sampel ini memverifikasi bahwa pesan memiliki kunci tipe String yang bukan null. Kemudian, partisioner tersebut mengembalikan 0 untuk pesan dengan kunci yang ditentukan dan nomor partisi yang bukan 0 untuk kunci lainnya, berdasarkan fungsi hashing standar.

Untuk meng-assign partition secara manual, gunakan kode berikut:

```
ProducerRecord<String, String> record
   = new ProducerRecord<String, String>("my_topic", 0, key, value);
```

Message akan dikirim ke partisi 0.
