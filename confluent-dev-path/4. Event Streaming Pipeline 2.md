# Membangun Event Streaming Pipeline
## Aplikasi Apache Kafka Streams

Kafka Streams adalah library klien Java untuk membangun aplikasi dan layanan mikro, di mana data input dan output disimpan dalam cluster Kafka.

![image](https://github.com/user-attachments/assets/ec22f394-b72d-4c82-8f0b-f0ff25a05f8d)

Sebuah stream mencakup seluruh masa lalu, masa kini, dan masa depan.

Dalam gambar di atas, kita melihat aliran event tersebut di sisi kiri. Setiap event merupakan debit atau kredit ke rekening bank. Seseorang bisa melihat seluruh riwayat dari stream, namun, jika hanya ingin melihat status saldo pada waktu tertentu, bisa gunakan table.

![image](https://github.com/user-attachments/assets/fa0c38c3-af75-4bcf-9de2-8c96a2f81dfb)

• Source Processor: Sebuah source processor adalah jenis khusus dari stream processor yang tidak memiliki prosesor hulu. Ia menghasilkan aliran input ke topologinya dari satu atau beberapa topik Kafka dengan menggunakan rekaman dari topik-topik ini dan meneruskannya ke prosesor hilirnya. 

• Sink Processor: Sebuah sink processor adalah jenis khusus dari stream processor yang tidak memiliki prosesor hilir. Ia mengirimkan record yang diterima dari prosesor hulunya ke topik Kafka tertentu.

![image](https://github.com/user-attachments/assets/a5287bf4-ebbe-4de4-acde-99558e3b2390)

Diagram ini menunjukkan aplikasi Kafka Streams yang memiliki beberapa thread stream, masing-masing dengan beberapa tugas. Thread dapat berjalan paralel di satu sistem atau didistribusikan untuk skalabilitas. Setiap thread memiliki konsumen dan produsen, serta tugas yang menyimpan status lokal di RocksDB, yang dipersistenkan ke topik changelog Kafka.

Windows:
- Membagi stream menjadi "time bucket"
- Tumbling, Hopping, dan Session Windows

![image](https://github.com/user-attachments/assets/2e174f1f-b447-4cf9-bdc9-1349b6e7fd2a)

Aggregations:
- Mengakumulasi sejumlah value saat rekaman baru masuk
- Biasanya berjendela
- Contoh: sum, count, max, min

Joins:
- Gabungkan stream/tabel yang berbeda bersama-sama pada sebuah kunci
- Contoh: Gabungkan aliran lokasi pengemudi dengan tabel profil pengemudi pada kunci ID pengemudi untuk membuat stream yang diperkaya dengan info lokasi dan profil
- Dapat dijendela dengan "sliding window"

### Contoh Kafka Streams (Konfigurasi)

```
public class SimpleStreamsExample {
   public static void main(String[] args) throws Exception {
      Properties config = new Properties();
      // Give the Streams application a unique name. The name must be unique in the Kafka cluster
      config.put(StreamsConfig.APPLICATION_ID_CONFIG, "simple-streams-example");
      config.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "broker-1:9092,broker-2:9092");
      config.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
      // Specify default (de)serializers for record keys and for record values.
      config.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.ByteArray().getClass());
      config.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());
  }
}
```

- Baris 8 menunjukkan konfigurasi khusus untuk consumer aplikasi Kafka Streams. Ini memungkinkan untuk melakukan hal-hal seperti menyetel pengaturan batch produser di aplikasi Kafka Streams.
- Baris 10 dan 12 menunjukkan kunci dan nilai SerDes. Dalam contoh ini, tipe data akan sama untuk pesan masuk dan keluar sehingga SerDes tidak perlu ditimpa oleh kode.

### Contoh Kafka Streams (Topology)

```
     StreamsBuilder builder = new StreamsBuilder();
  
     // Construct a KStream from the input Topic "TextLinesTopic"
     KStream<byte[], String> textLines = builder.stream("TextLinesTopic");
     // Convert to upper case
     KStream<byte[], String> uppercasedWithMapValues =
     textLines.mapValues(value -> value.toUpperCase());
     // Write the results to a new Kafka Topic called "UppercasedTextLinesTopic".
     uppercasedWithMapValues.to("UppercasedTextLinesTopic");
     // Run the Streams application via `start()`
     streams.start();
     KafkaStreams streams = new KafkaStreams(builder.build(), config);
     // Stop the application gracefully
     Runtime.getRuntime().addShutdownHook(new Thread(streams::close));
  }
}
```

Objek KStream textLines dibuat dengan menggunakan topik TextLinesTopic. textLines diubah oleh mapValues ​​untuk mengubah semua karakter dalam pesan menjadi huruf kapital, sehingga menghasilkan objek KStream yang disebut uppercasedWithMapValues. KStream yang dihasilkan kemudian diproduksi ke topik UppercasedTextLinesTopic.

## Kafka Connect

![image](https://github.com/user-attachments/assets/5e442ef0-218f-4b5e-9f76-177faff8c676)

Kafka Connect adalah kerangka kerja untuk streaming data antara Apache Kafka dan sistem data lainnya. 

- Connector: pekerjaan logis yang menyalin data antara Kafka dan sistem lain

- Source Connector: membaca data dari sistem data eksternal ke Kafka 

   - Secara internal, konektor sumber menggunakan Kafka Producer
  
- Sink Connector: menulis data Kafka ke sistem data eksternal

   - Secara internal, konektor sink menggunakan Kafka Consumer Group

![image](https://github.com/user-attachments/assets/ed3cbc0f-a882-4b2d-8b81-4a895ffd6528)

Contoh kasus penggunaan untuk Kafka Connect meliputi:
- Mengalirkan seluruh basis data SQL ke Kafka
   - Bulk - memuat seluruh tabel
   - Change Data Capture (CDC) - memuat perubahan tabel saat terjadi
- Mengimpor file CSV yang dihasilkan oleh aplikasi lama ke Kafka
- Mengalirkan Topik Kafka ke Hadoop File System (HDFS) untuk pemrosesan batch
- Mengalirkan Topik Kafka ke Elasticsearch untuk pengindeksan sekunder
- Mengarsipkan data lama dalam penyimpanan objek berbiaya rendah
   - misalnya Amazon Simple Storage Service (S3)

Gunakan klien confluent-hub yang disertakan dengan Confluent Platform untuk menginstall salah satu connector dari Confluent Hub:

```
confluent-hub install debezium/debezium-connector-mysql:latest
```

![image](https://github.com/user-attachments/assets/3d4ced49-ad3c-474f-80f1-baf0281a924e)

- Membagi beban kerja menjadi bagian-bagian yang lebih kecil memberikan paralelisme dan skalabilitas
- Pekerjaan konektor dipecah menjadi task-task yang melakukan penyalinan data yang sebenarnya
- Worker adalah proses yang menjalankan satu atau lebih tugas, masing-masing dalam utas yang berbeda


![image](https://github.com/user-attachments/assets/48e9e1c1-f34f-4f3a-acdb-a9f33b61a5c9)

Kafka Connect, saat dijalankan dalam mode terdistribusi, secara otomatis dan transparan melakukan failover. Di sini ada 4 pekerja dengan 3 konektor A, B, C. Konektor A memiliki 4 tugas yang berjalan pada pekerja 1 dan 2 sementara konektor B dan C masing-masing memiliki satu tugas yang berjalan pada pekerja 3 dan 4.

![image](https://github.com/user-attachments/assets/8434eb24-851a-434e-829f-312b20f71c9c)

Jika Worker 4 gagal, task akan di-rebalance ke worker-worker yang tersisa. Dalam kasus ini, job konektor B telah dipindahkan ke pekerja 2 sementara job konektor C dan task C1 telah dipindahkan ke pekerja 3.

Berikut adalah konfigurasi penting untuk worker:

| Parameter          | Deskripsi                                                                                          |
|--------------------|---------------------------------------------------------------------------------------------------|
| bootstrap.servers  | Sebuah daftar pasangan host/port yang digunakan untuk membangun koneksi awal ke cluster Kafka.    |
| key.converter       | Kelas konverter untuk kunci.                                                                      |
| value.converter     | Kelas konverter untuk nilai.                                                                       |
| group.id           | Sebuah string unik yang mengidentifikasi grup cluster Kafka Connect tempat pekerja mode distribusi berada. |

Berikut adalah endpoint REST:

| Metode | Path                            | Deskripsi                                                    |
|--------|-----------------------------------|-------------------------------------------------------------|
| GET    | /connectors                       | Mendapatkan daftar konektor aktif                          |
| POST   | /connectors                       | Membuat konektor baru                                       |
| GET    | /connectors/(string: name)/config | Mendapatkan informasi konfigurasi untuk sebuah konektor     |
| PUT    | /connectors/(string: name)/config | Membuat konektor baru atau memperbarui konfigurasi konektor yang sudah ada |

Konfigurasi Connector:

```
{
  "name": "Driver-Connector",
  "config": {
    "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
    "connection.url": "jdbc:postgresql://postgres:5432/postgres",
    "connection.user": "postgres",
    "table.whitelist": "driver",
    "topic.prefix": "my-postgres-",
    "mode": "timestamp+incrementing",
    "incrementing.column.name": "id",
    "timestamp.column.name": "timestamp",
    "table.types": "TABLE",
    "numeric.mapping": "best_fit"
  }
}
```

Konektor dapat ditambahkan, dimodifikasi, dan dihapus melalui REST API pada port 8083. Dalam mode terdistribusi, konfigurasi hanya dapat dilakukan melalui REST API. Dalam mode standalone, konfigurasi juga dapat dilakukan melalui REST API. Namun, biasanya konfigurasi dilakukan melalui file properti.

![image](https://github.com/user-attachments/assets/d85e4116-8e5d-45ed-b490-0c3f15279d34)

Transformasi pesan tunggal (SMT) adalah bagian penting dari jalur pemrosesan Kafka Connect, terutama untuk konektor sumber. Berikut adalah cara kerjanya dalam proses:

- Konektor Sumber: Membaca data dari sistem eksternal dan menyimpannya di Kafka.
- Terjemahan Data: Tugas dari konektor membaca data dalam format aslinya dan mengubahnya menjadi API data generik Kafka Connect.
- Transformasi: Setelah berada dalam format generik, SMT dapat diterapkan, memungkinkan transformasi apa pun tanpa memandang format data asli.
- Penyelesaian: Data yang telah ditransformasi tetap dalam format generik dan kemudian diserialisasi menjadi Avro atau JSON oleh konverter sebelum dikirim ke Kafka.
- Dapat Digunakan Kembali: Proses ini memungkinkan transformasi umum yang dapat diterapkan pada konektor mana pun, mirip dengan cara format serialisasi dapat digunakan kembali di seluruh konektor.

| **Operasi**      | **Deskripsi**                                                                                      |
|------------------|---------------------------------------------------------------------------------------------------|
| InsertField      | Menyisipkan sebuah field menggunakan atribut dari metadata pesan atau dari nilai statis yang dikonfigurasi. |
| ReplaceField     | Mengganti nama field, atau menerapkan daftar untuk memfilter.                                     |
| MaskField        | Mengganti field dengan nilai null yang valid untuk tipe tersebut (0, string kosong, dll).         |
| ValueToKey       | Mengganti kunci dengan kunci baru yang dibentuk dari subset field dalam payload nilai.           |
| HoistField       | Mengemas seluruh event sebagai satu field di dalam Struct atau Map.                              |
| ExtractField     | Mengekstrak field tertentu dari Struct dan Map dan hanya menyertakan field ini dalam hasil.      |


![image](https://github.com/user-attachments/assets/d160df3f-0dbc-4bcb-b87b-b0b7ff46fe22)

- Konverter menyediakan format data yang ditulis atau dibaca dari Kafka (seperti Serializer)
- Konverter dipisahkan dari Konektor sehingga dapat digunakan kembali
   - Memungkinkan konektor apa pun bekerja dengan format serialisasi apa pun
 
Konverter berlaku untuk kunci dan nilai pesan
- Konverter kunci dan nilai dapat diatur secara independen
   - key.converter
   - value.converter
- Format data yang telah ditetapkan sebelumnya untuk Konverter
   - Avro: AvroConverter
   - Byte Array: ByteArrayConverter
   - JSON: JsonConverter
   - Skema JSON: JsonSchemaConverter
   - Protobuf: ProtobufConverter
   - String: StringConverter
 
Contoh:

```
key.converter=io.confluent.connect.avro.AvroConverter
key.converter.schema.registry.url=http://schemaregistry1:8081
value.converter=io.confluent.connect.avro.AvroConverter
value.converter.schema.registry.url=http://schemaregistry1:8081
```

## KSqlDB


