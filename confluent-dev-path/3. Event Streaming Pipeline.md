# Membangun Event Streaming Pipeline

## Produce Message ke Kafka

Dalam pembelajaran ini, saya akan menggunakan java untuk membuat producer. Pertama, buat properties seperti berikut:

```
final Properties props = new Properties(); 
props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "broker-1:9092,broker-2:9092");
... // Load other properties
KafkaProducer<String, MyObject> producer = new KafkaProducer<>(props);
```

Dalam produksi, penting untuk memisahkan konfigurasi dari logika aplikasi agar logika yang sama dapat dijalankan di berbagai lingkungan dengan properti yang berbeda. Dalam Java, hal ini biasanya dilakukan dengan menggunakan file .properties, di mana properti didefinisikan dengan simbol "=". Klien Java Kafka menggunakan pendekatan ini, memungkinkan fleksibilitas dalam mendefinisikan konfigurasi seperti bootstrap.servers, contohnya `bootstrap.servers=broker-1:9092,broker-2:9092`.

```
 String k = "mykey";
 String v = "myvalue";
 ProducerRecord<String, String> record = new ProducerRecord<String, String>(
 "my_topic", k, v);
 producer.send(record);
```

Metode Producer.send() segera menambahkan pesan ke buffer lokal untuk pengiriman tertunda, memungkinkan pemrosesan batch untuk meningkatkan kinerja. Pesan kemudian dikirim ke broker berdasarkan konfigurasi batching atau secara manual menggunakan metode flush(). Sebuah thread latar belakang internal mendorong catatan ke broker, dipicu oleh ambang batas batching dan menunggu konfirmasi (sesuai pengaturan acks). Metode flush() manual mendorong semua antrian sekaligus secara sinkron, sedangkan thread latar belakang menangani mereka secara individu.

Konstruktor ProducerRecord membungkus kunci dan nilai yang diberikan menjadi sebuah pesan, lengkap dengan header. Saat send() dipanggil, KafkaProducer melakukan serialisasi kunci dan nilai, lalu menggunakan partitioner default untuk menentukan partisi di topik tempat pesan akan disimpan, berdasarkan data dalam ProducerRecord.

```
producer.send(record, (recordMetadata, e) -> {
  if (e != null) {
    e.printStackTrace();    
  } else {   
    System.out.println("Message String = " + record.value() +
     ", Offset = " + recordMetadata.offset());                     
  }
});
```

Untuk menghubungkan callback dengan catatan tertentu (misalnya, jika send() gagal), Anda dapat menggunakan informasi yang tersedia dalam closure callback (fungsi yang dapat diteruskan sebagai objek dan memiliki akses ke variabel yang sebelumnya berada dalam cakupan). Jika tidak menggunakan fungsi lambda, informasi ini dapat diteruskan langsung ke konstruktor callback.

## Consume Message dari Kafka

Berikut adalah properties penting untuk consumer:

| **Properti**              | **Deskripsi**                                                                 |
|---------------------------|-------------------------------------------------------------------------------|
| `bootstrap.servers`        | Daftar pasangan host/port Broker yang digunakan untuk menghubungkan awal ke cluster. |
| `key.deserializer`         | Kelas yang digunakan untuk mendeserialisasi kunci. Harus mengimplementasikan antarmuka `Deserializer`. |
| `value.deserializer`       | Kelas yang digunakan untuk mendeserialisasi nilai. Harus mengimplementasikan antarmuka `Deserializer`. |
| `group.id`                 | String unik yang mengidentifikasi Grup Konsumen di mana Konsumen ini tergabung. |
| `enable.auto.commit`       | Jika diatur ke `true` (default), Konsumen akan memicu commit offset berdasarkan nilai dari `auto.commit.interval.ms` (default 5000ms). |

Buat dalam file consumer, isi dengan properties tersebut:

```
Properties props = new Properties();
props.put("bootstrap.servers", "broker-1:9092,broker-2:9092");
props.put("group.id", "java-consumer");
props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer.class");
props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer.class"); 
KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
```

- ByteArraySerializer, IntegerSerializer, LongSerializer, dan lainnya sudah termasuk dalam client Kafka.
- StringSerializer secara default menggunakan encoding UTF-8. Encoding ini dapat dikustomisasi dengan mengatur properti serializer.encoding.

Subscribe ke topic dengan kode berikut:

```
consumer.subscribe(Arrays.asList("my_topic", "my_other_topic"))
```

Dengan kode di atas, consumer akan subscribe ke topic yang lebih dari satu, yaitu my_topic dan my_other_topic. Setelah itu, buat polling:

```
try {
  while (true) {
    ConsumerRecords<String, MyObject> records = consumer.poll(Duration.ofMillis(100));
    for (ConsumerRecord<String, MyObject> record : records)  
      System.out.printf("offset = %d, key = %s, value = %s\n", record.offset(), record.key(), record.value());
  }
 } finally { 
// avoid resource leaks
 consumer.close();
}
```

Metode poll(Duration) memiliki banyak fungsi, tetapi yang utama untuk diketahui adalah:

• Mengambil record dari partisi yang ditetapkan

• Memicu penugasan partisi (jika perlu)

• Melakukan komit offset konsumen jika komit offset otomatis diaktifkan dan `interval auto.commit.ms` terlampaui

> Sebagai praktik terbaik, bungkus kode dalam blok try{ }
> 
> Untuk menghindari kebocoran sumber daya, tutup konsumen dalam blok `finally{ }`
> 
> Warning: KafkaConsumer tidak aman untuk thread. Artinya objek KafkaConsumer tidak dapat diakses atau digunakan oleh beberapa thread secara bersamaan.

## Produce dan Consume Message menggunakan Avro

![image](https://github.com/user-attachments/assets/0f3b8a28-61c6-4936-96a8-304723d37e88)

Skema data adalah apa yang menggambarkan struktur data. Skema ini dapat digunakan sebagai kontrak antara dua pihak, produser data dan konsumer data. Dengan data ini, konsumer selalu tahu apa yang diharapkan dari sisi produser.

Jika tidak ada skema data, maka tidak ada cara untuk mengembangkan struktur data dari waktu ke waktu dengan cara yang terkendali. Ini disebut evolusi skema.

![image](https://github.com/user-attachments/assets/94c95bed-f0e0-4ba6-9696-7dbfc766b355)

• Serialisasi adalah cara untuk merepresentasikan data dalam memori sebagai serangkaian byte

> Diperlukan untuk mentransfer data melalui jaringan, atau menyimpannya di disk

• Deserialisasi adalah proses mengubah aliran byte kembali menjadi objek data

Serialisasi penting karena Kafka menggunakan array byte sebagai format untuk mengangkut dan menyimpan data pada broker. Namun, serializer dan deserializer tidak standar di seluruh sistem operasi dan bahasa pemrograman. Kafka mencoba untuk mengurangi masalah ini dengan menyediakan kelas serialisasi tetapi ini didasarkan pada tipe sederhana dan mungkin tidak cukup untuk tipe data yang lebih kompleks.

### Avro

Skema Avro menentukan struktur data

• Skema direpresentasikan dalam format JSON atau IDL

• Avro memiliki tiga cara berbeda untuk membuat rekaman:

| **Pendekatan**  | **Langkah-Langkah**                                                                 | **Kegunaan**                                                              |
|-----------------|-------------------------------------------------------------------------------------|---------------------------------------------------------------------------|
| **Generic**     | 1. Tulis skema secara manual <br> 2. Buat objek `GenericRecord` dari skema secara manual | Digunakan untuk skema yang sering berubah atau sangat bervariasi.          |
| **Reflection**  | 1. Buat tipe data secara manual <br> 2. Hasilkan skema dari kode                        |                                                                           |
| **Specific**    | 1. Tulis skema secara manual <br> 2. Hasilkan kelas dari skema untuk disertakan dalam program | Cara paling umum untuk menggunakan kelas Avro.                             |

Agar skema Avro bisa digunakan, harus memiliki tipe data dalam bahasa pemrograman pilihan dan skema untuk mendeskripsikannya dalam format JSON demi alasan kompatibilitas.

Contoh Skema Avro:

```
{
  "namespace": "com.traincompany.examples",
  "type": "record",
  "name": "TrainArrived",
  "fields": [
    { "name": "trainId", "type": "int" },
    { "name": "stationId", "type": "int" },
    {
      "name": "arrivalTime",
      "type": "int",
      "doc": "Time in ms since the epoch"
    }
  ]
}
```

> `doc` memungkinkan untuk menempatkan komentar dalam definisi skema

### Confluent Schema Registry

![image](https://github.com/user-attachments/assets/cc90037d-71c0-42f7-86d7-a199914b5ade)

Kegunaan Confluent Schema Registry:
- menyediakan manajemen skema terpusat, menyimpan riwayat versi semua skema, dan menyediakan antarmuka RESTful untuk menyimpan serta mengambil skema Avro.
- Memeriksa kesesuaian data dengan skema dan membuang pengecualian jika tidak sesuai.
- Memungkinkan evolusi skema sesuai dengan pengaturan kompatibilitas yang dikonfigurasi.
- Mengirim skema Avro dengan setiap pesan tidak efisien; sebagai gantinya, ID unik global dari skema Avro dikirim bersama setiap pesan.

Informasi skema disimpan dalam topik Kafka khusus dan dapat diakses melalui REST API dan Java API, serta alat baris perintah seperti kafka-avro-console-producer dan kafka-avro-console-consumer.

Topik Kafka khusus (nama default adalah "_schemas") diperlukan dan dapat dikonfigurasi ulang dengan parameter kafkastore.topic.

#### REST API Schema Registry (POST) 

Register schema untuk key:

```
 # Register a schema for keys
 $ curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \--data '{"schema": "{\"type\": \"string\"}"}' \
    http://schemaregistry1:8081/subjects/<topic-name>-key/versions
 HTTP/1.1 200 OK
 Content-Type: application/vnd.schemaregistry.v1+json
 {"id":1}
```

Register Schema untuk value:

```
 # Register another schema for values
 $ curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \--data '{"schema": "{\"type\": \"string\"}"}' \
    http://schemaregistry1:8081/subjects/<topic-name>-value/versions
 HTTP/1.1 200 OK
 Content-Type: application/vnd.schemaregistry.v1+json
 {"id":2}
```

#### REST API Schema Registry (GET) 

Get schema dengan id=1:

```
 # Retrieve the schema with id 1
 $ curl -X GET http://schemaregistry1:8081/schemas/ids/1
 HTTP/1.1 200 OK
 Content-Type: application/vnd.schemaregistry.v1+json
 {"schema": "{\"type\": \"string\"}"}
```

Cek semua subject schema yang disimpan di schema registry:

 ```
 # Check all the different schema subjects stored in Schema Registry
 $ curl -X GET http://schemaregistry1:8081/subjects
 HTTP/1.1 200 OK
 Content-Type: application/vnd.schemaregistry.v1+json
 ["my_topic-key", "my_topic-value"]
```

Strategi Penamaan Subjek
• Subjek: ruang lingkup tempat skema dapat berkembang di Schema Registry

| Strategi Penamaan               | Konfigurasi                     |
|----------------------------------|----------------------------------|
| TopicNameStrategy (default)      | key.subject.name.strategy       |
| RecordNameStrategy                | value.subject.name.strategy     |
| TopicRecordNameStrategy           |                                  |

Jenis Kompatibilitas Skema:

| Tipe Kompatibilitas | Resolusi Skema                                                        | Upgrade Pertama                    |
|---------------------|----------------------------------------------------------------------|------------------------------------|
| BACKWARD            | Konsumen yang menggunakan skema baru dapat membaca data yang diproduksi dengan skema sebelumnya. | Consumee                           |
| FORWARD             | Konsumen yang menggunakan skema sebelumnya dapat membaca data yang diproduksi dengan skema baru.  | Producer                          |
| FULL                | Kombinasi dari kompatibilitas BACKWARD dan FORWARD.                 | Semua urutan                      |
| NONE                | Pemeriksaan kompatibilitas dinonaktifkan.                            | Semua urutan                      |

### Java Avro

Producer:

![image](https://github.com/user-attachments/assets/09e27457-72ae-41d9-80f6-2d1969fc57ab)

Pada baris ke-5, sudah ada kelas KafkaAvroSerializer, yang akan digunakan untuk membuat serial nilai pesan. Pada baris ke-6 dan ke-7, konfigurasi produser untuk menggunakan Schema Registry.

Consumer:

![image](https://github.com/user-attachments/assets/9f312b20-ba3f-4efb-a961-bf75ff276f62)

Dalam kode ini dan kode produser pada halaman sebelumnya, tampaknya tidak ada metode yang berkomunikasi langsung dengan Schema Registry. Interaksi dengan Schema Registry diabstraksikan ke dalam KafkaAvroSerializer dan KafkaAvroDeserializer. Baris 7 diperlukan untuk perilaku yang disukai dari pembuatan kode "spesifik". Jika baris ini dihilangkan, default "generik" digunakan sebagai gantinya.

