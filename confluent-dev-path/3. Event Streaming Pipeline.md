# Membangun Event Streaming Pipeline

## Produce Message ke Kafka

Dalam pembelajaran ini, saya akan menggunakan java untuk membuat producer. Pertama, buat properties seperti berikut:

```
final Properties props = new Properties(); 
props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "broker-1:9092,broker-2:9092");
... // Load other properties
KafkaProducer<String, MyObject> producer = new KafkaProducer<>(props);
```

Dalam produksi, penting untuk memisahkan konfigurasi dari logika aplikasi agar logika yang sama dapat dijalankan di berbagai lingkungan dengan properti yang berbeda. Dalam Java, hal ini biasanya dilakukan dengan menggunakan file .properties, di mana properti didefinisikan dengan simbol "=". Klien Java Kafka menggunakan pendekatan ini, memungkinkan fleksibilitas dalam mendefinisikan konfigurasi seperti bootstrap.servers, contohnya `bootstrap.servers=broker-1:9092,broker-2:9092`.

```
 String k = "mykey";
 String v = "myvalue";
 ProducerRecord<String, String> record = new ProducerRecord<String, String>(
 "my_topic", k, v);
 producer.send(record);
```

Metode Producer.send() segera menambahkan pesan ke buffer lokal untuk pengiriman tertunda, memungkinkan pemrosesan batch untuk meningkatkan kinerja. Pesan kemudian dikirim ke broker berdasarkan konfigurasi batching atau secara manual menggunakan metode flush(). Sebuah thread latar belakang internal mendorong catatan ke broker, dipicu oleh ambang batas batching dan menunggu konfirmasi (sesuai pengaturan acks). Metode flush() manual mendorong semua antrian sekaligus secara sinkron, sedangkan thread latar belakang menangani mereka secara individu.

Konstruktor ProducerRecord membungkus kunci dan nilai yang diberikan menjadi sebuah pesan, lengkap dengan header. Saat send() dipanggil, KafkaProducer melakukan serialisasi kunci dan nilai, lalu menggunakan partitioner default untuk menentukan partisi di topik tempat pesan akan disimpan, berdasarkan data dalam ProducerRecord.

```
producer.send(record, (recordMetadata, e) -> {
  if (e != null) {
    e.printStackTrace();    
  } else {   
    System.out.println("Message String = " + record.value() +
     ", Offset = " + recordMetadata.offset());                     
  }
});
```

Untuk menghubungkan callback dengan catatan tertentu (misalnya, jika send() gagal), Anda dapat menggunakan informasi yang tersedia dalam closure callback (fungsi yang dapat diteruskan sebagai objek dan memiliki akses ke variabel yang sebelumnya berada dalam cakupan). Jika tidak menggunakan fungsi lambda, informasi ini dapat diteruskan langsung ke konstruktor callback.

## Consume Message dari Kafka

Berikut adalah properties penting untuk consumer:

| **Properti**              | **Deskripsi**                                                                 |
|---------------------------|-------------------------------------------------------------------------------|
| `bootstrap.servers`        | Daftar pasangan host/port Broker yang digunakan untuk menghubungkan awal ke cluster. |
| `key.deserializer`         | Kelas yang digunakan untuk mendeserialisasi kunci. Harus mengimplementasikan antarmuka `Deserializer`. |
| `value.deserializer`       | Kelas yang digunakan untuk mendeserialisasi nilai. Harus mengimplementasikan antarmuka `Deserializer`. |
| `group.id`                 | String unik yang mengidentifikasi Grup Konsumen di mana Konsumen ini tergabung. |
| `enable.auto.commit`       | Jika diatur ke `true` (default), Konsumen akan memicu commit offset berdasarkan nilai dari `auto.commit.interval.ms` (default 5000ms). |

Buat dalam file consumer, isi dengan properties tersebut:

```
Properties props = new Properties();
props.put("bootstrap.servers", "broker-1:9092,broker-2:9092");
props.put("group.id", "java-consumer");
props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer.class");
props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer.class"); 
KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
```

- ByteArraySerializer, IntegerSerializer, LongSerializer, dan lainnya sudah termasuk dalam client Kafka.
- StringSerializer secara default menggunakan encoding UTF-8. Encoding ini dapat dikustomisasi dengan mengatur properti serializer.encoding.

Subscribe ke topic dengan kode berikut:

```
consumer.subscribe(Arrays.asList("my_topic", "my_other_topic"))
```

Dengan kode di atas, consumer akan subscribe ke topic yang lebih dari satu, yaitu my_topic dan my_other_topic. Setelah itu, buat polling:

```
try {
  while (true) {
    ConsumerRecords<String, MyObject> records = consumer.poll(Duration.ofMillis(100));
    for (ConsumerRecord<String, MyObject> record : records)  
      System.out.printf("offset = %d, key = %s, value = %s\n", record.offset(), record.key(), record.value());
  }
 } finally { 
// avoid resource leaks
 consumer.close();
}
```

Metode poll(Duration) memiliki banyak fungsi, tetapi yang utama untuk diketahui adalah:

• Mengambil record dari partisi yang ditetapkan
• Memicu penugasan partisi (jika perlu)
• Melakukan komit offset konsumen jika komit offset otomatis diaktifkan dan `interval auto.commit.ms` terlampaui

- Sebagai praktik terbaik, bungkus kode dalam blok try{ }
- Untuk menghindari kebocoran sumber daya, tutup konsumen dalam blok `finally{ }`
- Warning: KafkaConsumer tidak aman untuk thread. Artinya objek KafkaConsumer tidak dapat diakses atau digunakan oleh beberapa thread secara bersamaan.
